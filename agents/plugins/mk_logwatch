#!/usr/bin/env python
# -*- encoding: utf-8; py-indent-offset: 4 -*-
# +------------------------------------------------------------------+
# |             ____ _               _        __  __ _  __           |
# |            / ___| |__   ___  ___| | __   |  \/  | |/ /           |
# |           | |   | '_ \ / _ \/ __| |/ /   | |\/| | ' /            |
# |           | |___| | | |  __/ (__|   <    | |  | | . \            |
# |            \____|_| |_|\___|\___|_|\_\___|_|  |_|_|\_\           |
# |                                                                  |
# | Copyright Mathias Kettner 2014             mk@mathias-kettner.de |
# +------------------------------------------------------------------+
#
# This file is part of Check_MK.
# The official homepage is at http://mathias-kettner.de/check_mk.
#
# check_mk is free software;  you can redistribute it and/or modify it
# under the  terms of the  GNU General Public License  as published by
# the Free Software Foundation in version 2.  check_mk is  distributed
# in the hope that it will be useful, but WITHOUT ANY WARRANTY;  with-
# out even the implied warranty of  MERCHANTABILITY  or  FITNESS FOR A
# PARTICULAR PURPOSE. See the  GNU General Public License for more de-
# tails. You should have  received  a copy of the  GNU  General Public
# License along with GNU Make; see the file  COPYING.  If  not,  write
# to the Free Software Foundation, Inc., 51 Franklin St,  Fifth Floor,
# Boston, MA 02110-1301 USA.

import glob
import logging
import os
import re
import shutil
import sys
import time

from collections import namedtuple  # from Python2.6, Python3.1 onwards

# logical: LogfilesConfig([file, ...], (level, compiled, [continuation pattern, ...], [rewrite pattern, ...]))
# types: LogfilesConfig[List[str], Tuple[str, sre.SRE_Pattern object, List[str], List[str]]]
LogfilesConfig = namedtuple('LogfilesConfig', 'files, patterns')

# logical: ClusterConfig(name, ips)
# types: ClusterConfig[str, List[str]]
ClusterConfig = namedtuple('ClusterConfig', 'name, ips')

LOGGER = logging.getLogger(__name__)


def parse_arguments(argv=None):
    """
    Custom argument parsing.
    (Neither use optparse which is Python 2.3 to 2.7 only.
    Nor use argparse which is Python 2.7 onwards only.)
    """
    args = {}
    if argv is None:
        argv = sys.argv[1:]
    if "-h" in argv:
        sys.stderr.write("""
This is the Check_MK Agent plugin. If configured it will be called by the
agent without arguments.

Options:
    -d    Debug mode: Colored output, no saving of status.
    -h    Show help.
    -v    Verbose output for debugging purposes (no debug mode).

You should find an example configuration file at
'../cfg_examples/logwatch.cfg' relative to this file.

""")
        sys.exit(0)
    if "-v" in argv:
        logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
    elif "-vv" in argv:
        logging.basicConfig(level=logging.DEBUG, format="%(levelname)s: %(lineno)s: %(message)s")
    else:
        LOGGER.propagate = False
    return args


#   .--MEI-Cleanup---------------------------------------------------------.
#   |     __  __ _____ ___       ____ _                                    |
#   |    |  \/  | ____|_ _|     / ___| | ___  __ _ _ __  _   _ _ __        |
#   |    | |\/| |  _|  | |_____| |   | |/ _ \/ _` | '_ \| | | | '_ \       |
#   |    | |  | | |___ | |_____| |___| |  __/ (_| | | | | |_| | |_) |      |
#   |    |_|  |_|_____|___|     \____|_|\___|\__,_|_| |_|\__,_| .__/       |
#   |                                                         |_|          |
#   +----------------------------------------------------------------------+
# In case the program crashes or is killed in a hard way, the frozen binary .exe
# may leave temporary directories named "_MEI..." in the temporary path. Clean them
# up to prevent eating disk space over time.

########################################################################
############## DUPLICATE CODE WARNING ##################################
### This code is also used in the cmk-update-agent frozen binary #######
### Any changes to this class should also be made in cmk-update-agent ##
### In the bright future we will move this code into a library #########
########################################################################


class MEIFolderCleaner(object):
    def pid_running(self, pid):
        import ctypes
        kernel32 = ctypes.windll.kernel32
        SYNCHRONIZE = 0x100000

        process = kernel32.OpenProcess(SYNCHRONIZE, 0, pid)

        if process != 0:
            kernel32.CloseHandle(process)
            return True
        return False

    def find_and_remove_leftover_folders(self, hint_filenames):
        if not hasattr(sys, "frozen"):
            return

        import win32file  # pylint: disable=import-error
        import tempfile
        base_path = tempfile.gettempdir()
        for f in os.listdir(base_path):
            try:
                path = os.path.join(base_path, f)

                if not os.path.isdir(path):
                    continue

                # Only care about directories related to our program
                invalid_dir = False
                for hint_filename in hint_filenames:
                    if not os.path.exists(os.path.join(path, hint_filename)):
                        invalid_dir = True
                        break
                if invalid_dir:
                    continue

                pyinstaller_tmp_path = win32file.GetLongPathName(sys._MEIPASS).lower()  # pylint: disable=no-member
                if pyinstaller_tmp_path == path.lower():
                    continue  # Skip our own directory

                # Extract the process id from the directory and check whether or not it is still
                # running. Don't delete directories of running processes!
                # The name of the temporary directories is "_MEI<PID><NR>". We try to extract the PID
                # by stripping of a single digit from the right. In the hope the NR is a single digit
                # in all relevant cases.
                pid = int(f[4:-1])
                if self.pid_running(pid):
                    continue

                shutil.rmtree(path)
            except Exception as e:
                LOGGER.debug("Finding and removing leftover folders failed: %s", e)


def debug():
    return '-d' in sys.argv[1:] or '--debug' in sys.argv[1:]


# The configuration file and status file are searched
# in the directory named by the environment variable
# LOGWATCH_DIR. If that is not set, MK_CONFDIR is used.
# If that is not set either, the current directory ist
# used.
def mk_vardir():
    return os.getenv("LOGWATCH_DIR") or os.getenv("MK_VARDIR") or os.getenv("MK_STATEDIR") or "."


def mk_confdir():
    """Note: When debugging this plugin on hosts this function will always return "." cause
    required sudo rights for execution of /usr/lib/check_mk_agent/plugins/mk_logwatch
    doesn't provide env vars."""
    return os.getenv("LOGWATCH_DIR") or os.getenv("MK_CONFDIR") or "."


def get_status_filename(config):
    """
    Side effect:
    - Depend on ENV var.
    - In case agent plugin is called with debug option set -> depends on global
      LOGGER and stdout.

    Determine the name of the state file dependent on ENV variable and config:
    $REMOTE set, no cluster set or no ip match -> logwatch.state.<formatted-REMOTE>
    $REMOTE set, cluster set and ip match      -> logwatch.state.<cluster-name>
    $REMOTE not set and a tty                  -> logwatch.state.local
    $REMOTE not set and not a tty              -> logwatch.state

    $REMOTE is determined by the check_mk_agent and varies dependent on how the
    check_mk_agent is accessed:
    - telnet ($REMOTE_HOST): $REMOTE is in IPv6 notation. IPv4 is extended to IPv6
                             notation e.g. ::ffff:127.0.0.1
    - ssh ($SSH_CLIENT): $REMOTE is either in IPv4 or IPv6 notation dependent on the
                         IP family of the remote host.

    <formatted-REMOTE> is REMOTE with colons (:) replaced with underscores (_) for
    IPv6 address, is to IPv6 notation extended address with colons (:) replaced with
    underscores (_) for IPv4 address or is plain $REMOTE in case it does not match
    an IPv4 or IPv6 address.
    """
    remote = os.getenv("REMOTE", "")
    ipv4_regex = r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$"
    ipv6_regex = r"^(?:[A-F0-9]{1,4}:){7}[A-F0-9]{1,4}$"
    ipv4_match = re.match(ipv4_regex, remote)
    ipv6_match = re.match(ipv6_regex, remote)
    remote_hostname = remote.replace(":", "_")
    if ipv4_match:
        remote_ip = ipv4_match.group()
    elif ipv6_match:
        remote_ip = ipv6_match.group()
        # in case of IPv4 extended to IPv6 get rid of prefix for ip match lookup
        if remote_ip.startswith("::ffff:"):
            remote_ip = remote_ip.replace("::ffff:", "")
    else:
        remote_ip = None
        LOGGER.debug("REMOTE neither IPv4 nor IPv6 address.")
    # In case cluster configured map ip to cluster name if configured.
    # key "name" is mandatory and unique for cluster dicts
    status_filename = None
    cluster_name = None
    cluster_configs = [nt for nt in config if isinstance(nt, ClusterConfig)]
    LOGGER.debug("Cluster configurations:")
    LOGGER.debug(cluster_configs)
    for nt in cluster_configs:
        for ip in nt.ips:
            if remote_ip == ip:
                # cluster name may not contain whitespaces (must be provided from
                # the WATO config as type ID or hostname).
                cluster_name = nt.name
                LOGGER.info("matching cluster ip %s", ip)
                LOGGER.info("matching cluster name %s", cluster_name)
    if cluster_name:
        status_filename = "%s/logwatch.state.%s" % (mk_vardir(), cluster_name)
    elif remote_hostname != "":
        status_filename = "%s/logwatch.state.%s" % (mk_vardir(), remote_hostname)
    elif remote_hostname == "" and sys.stdout.isatty():  # real terminal
        status_filename = "%s/logwatch.state.local" % mk_vardir()
    elif remote_hostname == "" and not sys.stdout.isatty():  # piped or redirected
        status_filename = "%s/logwatch.state" % mk_vardir()
    else:
        raise Exception("Status filename could not be determined.")
    LOGGER.info("Status filename: %s", status_filename)
    return status_filename


def os_type():
    try:
        import platform  # added in Python 2.3
        return platform.system().lower()
    except ImportError:
        return "linux"


def is_comment(line):
    return line.lstrip().startswith('#')


def is_empty(line):
    return line.strip() == ""


def is_pattern_or_ip(line):
    return line.startswith(" ")


def parse_filenames(line):
    return line.split()


def parse_pattern(level, pattern, line):
    if level not in ['C', 'W', 'I', 'O']:
        raise Exception("Invalid pattern line '%s'" % line)
    try:
        compiled = re.compile(pattern)
    except:
        raise Exception("Invalid regular expression in line '%s'" % line)
    return (level, compiled)


def get_config_files(directory):
    config_file_paths = []
    config_file_paths.append(directory + "/logwatch.cfg")
    # Add config file paths from a logwatch.d folder
    for config_file in glob.glob(directory + "/logwatch.d/*.cfg"):
        config_file_paths.append(config_file)
    LOGGER.info("Configuration file paths:")
    LOGGER.info(config_file_paths)
    return config_file_paths


def read_config(files):
    """
    Read logwatch.cfg (patterns, cluster mapping, etc.).

    Side effect: Reads filesystem files logwatch.cfg and /logwatch.d/*.cfg

    Returns configuration as list. List elements are namedtuples.
    Namedtuple either describes logile patterns and is LogfilesConfig(files, patterns).
    Or tuple describes optional cluster mapping and is ClusterConfig(name, ips)
    with ips as list of strings.
    """
    LOGGER.debug("config files:")
    LOGGER.debug(files)

    config = []
    logfiles_configs = []
    cluster_configs = []
    config_lines = []
    for f in files:
        try:
            config_lines += [line.rstrip() for line in open(f).readlines() if not is_comment(line)]
        except IOError:
            if debug():
                raise

    patterns = None
    cont_list = []
    rewrite_list = []
    # only cluster_mapping context requires parse context
    section_context = "no_section"  # valid: "no_section", "logfiles", "cluster"
    cluster_ips = []
    # parsing has to consider the following possible lines:
    # - comment lines (begin with #)
    # - logfiles line (begin not with #, are not empty and do not contain CLUSTER)
    # - cluster lines (begin with CLUSTER)
    # - logfiles patterns (follow logfiles lines, begin with whitespace)
    # - cluster ips (follow cluster lines, begin with whitespace)
    # Needs to consider end of lines to append ips to clusters as well.
    for line in config_lines:
        if is_comment(line):
            continue  # skip comments
        if is_empty(line):
            section_context = "no_section"
            continue  # go to next line
        if section_context == "no_section":
            if not line.startswith("CLUSTER") and not is_comment(line):
                section_context = "logfiles"
                patterns = []
                cont_list = []  # Clear list of continuation patterns from last file
                rewrite_list = []  # Same for rewrite patterns
                filenames = parse_filenames(line)
                logfiles_configs.append(LogfilesConfig(filenames, patterns))
                LOGGER.debug("filenames: %s", filenames)
            elif line.startswith("CLUSTER"):
                section_context = "cluster"
                cluster_ips = []
                cluster_name = line.replace("CLUSTER ", "").strip()
                cluster_configs.append(ClusterConfig(cluster_name, cluster_ips))
                LOGGER.debug("new cluster: %s", cluster_name)
            else:
                raise Exception("Parsing error. (section: %s, line: %s)" % (section_context, line))
        elif section_context == "logfiles":
            if not is_pattern_or_ip(line):
                raise Exception("Parsing error. (section: %s, line: %s)" % (section_context, line))
            LOGGER.debug("pattern line: %s", line)
            if patterns is None:
                raise Exception("Missing logfile names")
            level, pattern = line.split(None, 1)

            if level == 'A':
                cont_list.append(parse_cont_pattern(pattern))
            elif level == 'R':
                rewrite_list.append(pattern)
            else:
                level, compiled = parse_pattern(level, pattern, line)
                # New pattern for line matching => clear continuation and rewrite patterns
                cont_list = []
                rewrite_list = []
                pattern = (level, compiled, cont_list, rewrite_list)
                patterns.append(pattern)
                LOGGER.debug("pattern %s", pattern)
        elif section_context == "cluster":
            if not is_pattern_or_ip(line):
                raise Exception("Parsing error. (section: %s, line: %s)" % (section_context, line))
            ip = line.strip()
            cluster_ips.append(ip)
            LOGGER.debug("cluster ip: %s", ip)
        else:
            raise Exception("Parsing error. (section: %s, line: %s)" % (section_context, line))

    config.extend(logfiles_configs)
    config.extend(cluster_configs)

    LOGGER.info("Logfiles configurations:")
    LOGGER.info(logfiles_configs)
    LOGGER.info("Optional cluster configurations:")
    LOGGER.info(cluster_configs)
    return config


def parse_cont_pattern(pattern):
    try:
        return int(pattern)
    except:
        try:
            return re.compile(pattern)
        except:
            if debug():
                raise
            raise Exception("Invalid regular expression in line '%s'" % pattern)


def read_status(file_name):
    """
    Support status files with the following structure:

    # LOGFILE         OFFSET    INODE
    /var/log/messages|7767698|32455445
    /var/test/x12134.log|12345|32444355

    Status file lines may not be empty but must contain | separated status meta data.
    """
    LOGGER.debug("Status file:")
    LOGGER.debug(file_name)
    if debug():
        return {}

    status = {}
    for line in open(file_name):
        inode = -1
        try:
            parts = line.split('|')
            filename = parts[0]
            offset = parts[1]
            if len(parts) >= 3:
                inode = parts[2]
        except Exception as e:
            raise Exception("Parsing of status file %s line \"%s\" failed: %s" \
                            % (file_name, line, e))
        try:
            status[filename] = int(offset), int(inode)
        except Exception as e:
            raise Exception("Parsing of status file %s line \"%s\" failed: %s" \
                            % (file_name, line, e))

    LOGGER.info("read status:")
    LOGGER.info(status)
    return status


def save_status(status, file_name):
    LOGGER.debug("save status:")
    LOGGER.debug("status: %s", status)
    LOGGER.debug("filename: %s", file_name)
    with open(file_name, "w") as f:
        for filename, (offset, inode) in status.items():
            f.write("%s|%d|%d\n" % (filename, offset, inode))


def next_line(file_handle, continuation_line):
    if continuation_line is not None:
        return continuation_line, None

    try:
        line = file_handle.next()
        # Avoid parsing of (yet) incomplete lines (when actual application
        # is just in the process of writing)
        # Just check if the line ends with a \n. This handles \n and \r\n
        if not line.endswith("\n"):
            begin_of_line_offset = file_handle.tell() - len(line)
            os.lseek(file_handle.fileno(), begin_of_line_offset, 0)
            return None, None
        return line, None
    except:
        return None, None


def is_inode_cabable(path):
    if "linux" in os_type():
        return True
    elif "windows" in os_type():
        volume_name = "%s:\\\\" % path.split(":", 1)[0]
        import win32api  # pylint: disable=import-error
        volume_info = win32api.GetVolumeInformation(volume_name)
        volume_type = volume_info[-1]
        return "ntfs" in volume_type.lower()
    return False


def process_logfile(logfile, patterns, opt, status):
    """
    Returns tuple of (logfile lines, warning and/or error indicator, warning and/or error lines, logfile lines (list) in case the file has never been seen before and
    None in case the logfile cannot be opened.
    """
    loglines = []

    if debug():
        tty_red = '\033[1;31m'
        tty_green = '\033[1;32m'
        tty_yellow = '\033[1;33m'
        tty_blue = '\033[1;34m'
        tty_normal = '\033[0m'
    else:
        tty_red = ''
        tty_green = ''
        tty_yellow = ''
        tty_blue = ''
        tty_normal = ''

    # Look at which file offset we have finished scanning
    # the logfile last time. If we have never seen this file
    # before, we set the offset to -1
    offset, prev_inode = status.get(logfile, (-1, -1))
    try:
        file_desc = os.open(logfile, os.O_RDONLY)
        if not is_inode_cabable(logfile):
            inode = 1  # Create a dummy inode
        else:
            inode = os.fstat(file_desc)[1]  # 1 = st_ino
    except Exception:
        if debug():
            raise
        return None
    loglines.append("[[[%s]]]\n" % logfile)

    # Seek to the current end in order to determine file size
    current_end = os.lseek(file_desc, 0, 2)  # os.SEEK_END not available in Python 2.4
    status[logfile] = current_end, inode

    # If we have never seen this file before, we just set the
    # current pointer to the file end. We do not want to make
    # a fuss about ancient log messages...
    if offset == -1:
        if not debug():
            return loglines
        else:
            offset = 0

    # If the inode of the logfile has changed it has appearently
    # been started from new (logfile rotation). At least we must
    # assume that. In some rare cases (restore of a backup, etc)
    # we are wrong and resend old log messages
    if prev_inode >= 0 and inode != prev_inode:
        offset = 0

    # Our previously stored offset is the current end ->
    # no new lines in this file
    if offset == current_end:
        return (loglines, False, [])  # loglines contain logfile name only

    # If our offset is beyond the current end, the logfile has been
    # truncated or wrapped while keeping the same inode. We assume
    # that it contains all new data in that case and restart from
    # offset 0.
    if offset > current_end:
        offset = 0

    # now seek to offset where interesting data begins
    os.lseek(file_desc, offset, 0)  # os.SEEK_SET not available in Python 2.4
    if "windows" in os_type():
        import io  # Available with python 2.6
        # Some windows files are encoded in utf_16
        # Peak the first two bytes to determine the encoding...
        peak_handle = os.fdopen(file_desc, "rb")
        first_two_bytes = peak_handle.read(2)
        use_encoding = None
        if first_two_bytes == "\xFF\xFE":
            use_encoding = "utf_16"
        elif first_two_bytes == "\xFE\xFF":
            use_encoding = "utf_16_be"

        os.lseek(file_desc, offset, 0)  # os.SEEK_SET not available in Python 2.4
        file_handle = io.open(file_desc, encoding=use_encoding)
    else:
        file_handle = os.fdopen(file_desc)

    worst = -1
    warnings = False
    warnings_and_errors = []
    lines_parsed = 0
    start_time = time.time()
    pushed_back_line = None

    while True:
        line, pushed_back_line = next_line(file_handle, pushed_back_line)
        if line is None:
            break  # End of file

        # Handle option maxlinesize
        if opt.maxlinesize is not None and len(line) > opt.maxlinesize:
            line = line[:opt.maxlinesize] + "[TRUNCATED]\n"

        lines_parsed += 1
        # Check if maximum number of new log messages is exceeded
        if opt.maxlines is not None and lines_parsed > opt.maxlines:
            warnings_and_errors.append("%s Maximum number (%d) of new log messages exceeded.\n" % (
                opt.overflow,
                opt.maxlines,
            ))
            worst = max(worst, opt.overflow_level)
            os.lseek(file_desc, 0, os.SEEK_END)  # skip all other messages
            break

        # Check if maximum processing time (per file) is exceeded. Check only
        # every 100'th line in order to save system calls
        if opt.maxtime is not None and lines_parsed % 100 == 10 \
            and time.time() - start_time > opt.maxtime:
            warnings_and_errors.append(
                "%s Maximum parsing time (%.1f sec) of this log file exceeded.\n" % (
                    opt.overflow,
                    opt.maxtime,
                ))
            worst = max(worst, opt.overflow_level)
            os.lseek(file_desc, 0, os.SEEK_END)  # skip all other messages
            break

        level = "."
        for lev, pattern, cont_patterns, replacements in patterns:
            matches = pattern.search(line[:-1])
            if matches:
                level = lev
                levelint = {'C': 2, 'W': 1, 'O': 0, 'I': -1, '.': -1}[lev]
                worst = max(levelint, worst)

                # Check for continuation lines
                for cont_pattern in cont_patterns:
                    if isinstance(cont_pattern, int):  # add that many lines
                        for _unused_x in range(cont_pattern):
                            cont_line, pushed_back_line = next_line(file_handle, pushed_back_line)
                            if cont_line is None:  # end of file
                                break
                            line = line[:-1] + "\1" + cont_line

                    else:  # pattern is regex
                        while True:
                            cont_line, pushed_back_line = next_line(file_handle, pushed_back_line)
                            if cont_line is None:  # end of file
                                break
                            elif cont_pattern.search(cont_line[:-1]):
                                line = line[:-1] + "\1" + cont_line
                            else:
                                pushed_back_line = cont_line  # sorry for stealing this line
                                break

                # Replacement
                for replace in replacements:
                    line = replace.replace('\\0', line.rstrip()) + "\n"
                    for nr, group in enumerate(matches.groups()):
                        line = line.replace('\\%d' % (nr + 1), group)

                break  # matching rule found and executed

        color = {'C': tty_red, 'W': tty_yellow, 'O': tty_green, 'I': tty_blue, '.': ''}[level]
        if debug():
            line = line.replace("\1", "\nCONT:")
        if level == "I":
            level = "."
        if opt.nocontext and level == '.':
            continue
        warnings_and_errors.append("%s%s %s%s\n" % (color, level, line[:-1], tty_normal))

    new_offset = os.lseek(file_desc, 0, 1)  # os.SEEK_CUR not available in Python 2.4
    status[logfile] = new_offset, inode

    # output all lines if at least one warning, error or ok has been found
    if worst > -1:
        warnings = True

    # Handle option maxfilesize, regardless of warning or errors that have happened
    if opt.maxfilesize is not None and (offset / opt.maxfilesize) < (new_offset / opt.maxfilesize):
        warnings_and_errors.append(
            "%sW Maximum allowed logfile size (%d bytes) exceeded for the %dth time.%s\n" %
            (tty_yellow, opt.maxfilesize, new_offset / opt.maxfilesize, tty_normal))
    return (loglines, warnings, warnings_and_errors)


class Options(object):
    """Options w.r.t. logfile patterns (not w.r.t. cluster mapping)."""
    MAP_OVERFLOW = {'C': 2, 'W': 1, 'I': 0, 'O': 0}
    MAP_BOOL = {'true': True, 'false': False}

    def __init__(self):
        self.maxfilesize = None
        self.maxlines = None
        self.maxtime = None
        self.maxlinesize = None
        self.regex = None
        self._overflow = None
        self.nocontext = None

    @property
    def overflow(self):
        return 'C' if self._overflow is None else self._overflow

    @property
    def overflow_level(self):
        return self.MAP_OVERFLOW[self.overflow]

    def update(self, other):
        for attr in (
                'maxfilesize',
                'maxlines',
                'maxtime',
                'maxlinesize',
                'regex',
                '_overflow',
                'nocontext',
        ):
            new = getattr(other, attr)
            if new is not None:
                setattr(self, attr, new)

    def set_opt(self, opt_str):
        try:
            key, value = opt_str.split('=', 1)
            if key in ('maxlines', 'maxlinesize', 'maxfilesize'):
                setattr(self, key, int(value))
            elif key in ('maxtime',):
                setattr(self, key, float(value))
            elif key == 'overflow':
                if value not in self.MAP_OVERFLOW.keys():
                    raise ValueError("Invalid overflow: %r (choose from %r)" % (
                        value,
                        self.MAP_OVERFLOW.keys(),
                    ))
                self._overflow = value
            elif key in ('regex', 'iregex'):
                self.regex = re.compile(value, re.I if key.startswith('i') else 0)
            elif key in ('nocontext',):
                try:
                    setattr(self, key, self.MAP_BOOL[value.lower()])
                except KeyError:
                    raise ValueError("Invalid %s: %r (choose from %r)" % (
                        key,
                        value,
                        self.MAP_BOOL.keys(),
                    ))
            else:
                raise ValueError("Invalid option: %r" % opt_str)
        except ValueError as e:
            if debug():
                raise
            sys.stdout.write("INVALID CONFIGURATION: %s\n" % e)
            sys.exit(1)


def parse_sections(config):
    """
    Returns dict with logfile name as key and either tuple of (patterns, options)
    or None (in case the file cannot be found) as value.
    """
    logfile_patterns = {}

    logfiles_configs = [c for c in config if isinstance(c, LogfilesConfig)]
    for filenames, patterns in logfiles_configs:

        # First read all the options like 'maxlines=100' or 'maxtime=10'
        opt = Options()
        for item in filenames:
            if '=' in item:
                opt.set_opt(item)

        # Then handle the file patterns
        for glob_pattern in (f for f in filenames if '=' not in f):

            logfiles = glob.glob(glob_pattern)  # TODO: discard dirs via filter
            if opt.regex is not None:
                logfiles = [f for f in logfiles if opt.regex.search(f)]
            if not logfiles:
                logfile_patterns[glob_pattern] = None
            for logfile in logfiles:
                present_patterns, present_options = logfile_patterns.get(logfile, ([], Options()))
                present_patterns.extend(patterns)
                present_options.update(opt)
                logfile_patterns[logfile] = (present_patterns, present_options)

    return logfile_patterns.items()


def main():

    parse_arguments()

    sys.stdout.write("<<<logwatch>>>\n")

    try:
        # This removes leftover folders which may be generated by crashing frozen binaries
        folder_cleaner = MEIFolderCleaner()
        folder_cleaner.find_and_remove_leftover_folders(hint_filenames=["mk_logwatch.exe.manifest"])
    except Exception as e:
        sys.stdout.write("ERROR WHILE DOING FOLDER: %s\n" % e)
        sys.exit(1)

    try:
        files = get_config_files(mk_confdir())
        config = read_config(files)
    except Exception as e:
        if debug():
            raise
        sys.stdout.write("CANNOT READ CONFIG FILE: %s\n" % e)
        sys.exit(1)

    status_filename = get_status_filename(config)
    # Copy the last known state from the logwatch.state when there is no status_filename yet.
    if not os.path.exists(status_filename) and os.path.exists("%s/logwatch.state" % mk_vardir()):
        shutil.copy("%s/logwatch.state" % mk_vardir(), status_filename)

    # Simply ignore errors in the status file.  In case of a corrupted status file we simply begin
    # with an empty status. That keeps the monitoring up and running - even if we might lose a
    # message in the extreme case of a corrupted status file.
    try:
        status = read_status(status_filename)
    except Exception as e:
        status = {}

    loglines = []
    at_least_warnings = False
    warnings_and_errors = []
    for logfile, meta_data in parse_sections(config):
        if isinstance(meta_data, tuple):
            (patterns, options) = meta_data
            # When debugging use option -d to prevent from misleading exceptions
            # due to side effects of process_logfiles().
            process_result = process_logfile(logfile, patterns, options, status)
            if isinstance(process_result, tuple):
                loglines, at_least_warnings, warnings_and_errors = process_result
            elif isinstance(process_result, list):
                loglines = process_result
            elif isinstance(process_result, None):
                sys.stdout.write('[[[%s:cannotopen]]]' % logfile)
            else:
                LOGGER.debug("Invalid logfile processing result %s", process_result)
        elif isinstance(meta_data, None):
            sys.stdout.write('[[[%s:missing]]]\n' % logfile)
        else:
            LOGGER.debug("Invalid parse metadata %s", meta_data)

        LOGGER.debug("Loglines:")
        LOGGER.debug(loglines)
        for l in loglines:
            sys.stdout.write(l)
        if at_least_warnings:
            LOGGER.debug("Warnings and errors:")
            LOGGER.debug(warnings_and_errors)
            for we in warnings_and_errors:
                sys.stdout.write(we)

    if not debug():
        save_status(status, status_filename)


if __name__ == "__main__":
    main()
