#!/usr/bin/env python3
# Copyright (C) 2019 tribe29 GmbH - License: GNU General Public License v2
# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and
# conditions defined in the file COPYING, which is part of this source code package.

import ast
import errno
import fcntl
import getopt
import glob
import grp
import json
import os
import pwd
import re
import shutil
import signal
import socket
import subprocess
import sys
import tempfile
import textwrap
import time
import traceback
from collections.abc import Callable, Generator, Iterator, Mapping, Sequence
from contextlib import contextmanager, ExitStack
from dataclasses import dataclass
from hashlib import md5
from pathlib import Path
from typing import Any, IO, NamedTuple, NewType, NoReturn, TypedDict

from Cryptodome.Cipher import AES, PKCS1_OAEP
from Cryptodome.Cipher._mode_cbc import CbcMode
from Cryptodome.PublicKey import RSA
from OpenSSL import crypto

import cmk.utils.daemon as daemon
import cmk.utils.render as render
import cmk.utils.schedule as schedule
import cmk.utils.store as store
from cmk.utils.exceptions import MKGeneralException, MKTerminate
from cmk.utils.paths import mkbackup_lock_dir

VERSION = "2.2.0i1"

# Some typed wrappers around OpenSSL.crypto, there are only Python 2 interface
# files available... :-/

################
# Utility Code #
################

g_state: dict[str, Any] | None = None


# The state file is in JSON format because it is 1:1 transfered
# to the Checkmk server through the Checkmk agent.
class State:
    def __init__(self, path: Path) -> None:
        self.path = path

    def save(self, new_attrs: Mapping[str, Any], update: bool = True) -> None:
        if update:
            state = _load_state(self.path)
        else:
            state = {}
        state.update(new_attrs)
        store.save_text_to_file(
            self.path,
            json.dumps(
                state,
                sort_keys=True,
                indent=4,
                separators=(",", ": "),
            ),
        )

    def load(self) -> dict[str, Any]:
        return _load_state(self.path)


def _load_state(path: Path) -> dict[str, Any]:
    global g_state
    if g_state is None:
        g_state = json.loads(path.read_text())

    return g_state


def add_output(s: str, state: State) -> None:
    state_content = state.load()
    state_content["output"] += s
    state.save(state_content, update=False)


# Is used to duplicate output from stdout/stderr to a the job log. This
# is e.g. used during "mkbackup backup" to store the output.
class Log:
    def __init__(self, fd: int, state: State) -> None:
        self.fd = fd
        self.state = state
        if self.fd == 1:
            self.orig = sys.stdout
            sys.stdout = self  # type: ignore[assignment]
        else:
            self.orig = sys.stderr
            sys.stderr = self  # type: ignore[assignment]

        self.color_replace = re.compile("\033\\[\\d{1,2}m", re.UNICODE)

    def __del__(self) -> None:
        if self.fd == 1:
            sys.stdout = self.orig
        else:
            sys.stderr = self.orig

    def write(self, data: str) -> None:
        self.orig.write(data)
        try:
            add_output(self.color_replace.sub("", data), self.state)
        except Exception as e:
            self.orig.write("Failed to add output: %s\n" % e)

    def flush(self) -> None:
        self.orig.flush()


g_stdout_log = None
g_stderr_log = None


def start_logging(state: State) -> None:
    global g_stdout_log, g_stderr_log
    g_stdout_log = Log(1, state)
    g_stderr_log = Log(2, state)


def stop_logging() -> None:
    global g_stdout_log, g_stderr_log
    g_stderr_log = None
    g_stdout_log = None


def log(s: str) -> None:
    msg = "{} {}\n".format(time.strftime("%Y-%m-%d %H:%M:%S"), s)
    sys.stdout.write(msg)


def verbose(s):
    # type (str) -> None
    if opt_verbose > 0:
        log(s)


def hostname() -> str:
    return socket.gethostname()


def load_certificate_pem(buf: bytes) -> crypto.X509:
    return crypto.load_certificate(crypto.FILETYPE_PEM, buf)


def dump_publickey_pem(pkey: crypto.PKey) -> bytes:
    return crypto.dump_publickey(crypto.FILETYPE_PEM, pkey)


def load_privatekey_pem(buf: bytes, passphrase: bytes) -> crypto.PKey:
    return crypto.load_privatekey(crypto.FILETYPE_PEM, buf, passphrase)


def dump_privatekey_pem(pkey: crypto.PKey) -> bytes:
    return crypto.dump_privatekey(crypto.FILETYPE_PEM, pkey)


def _ensure_lock_file(lock_file_path: Path) -> None:
    # Create the file (but ensure that the target file always has the
    # correct permissions). The current lock implementation assumes the file is
    # kept after the process frees the lock (terminates). The pure existence of
    # the lock file is not locking anything. We work with fcntl.flock to realize
    # the locking.
    if lock_file_path.exists():
        return
    try:
        with tempfile.NamedTemporaryFile(
            mode="ab+",
            dir=str(lock_file_path.parent),
            delete=False,
        ) as backup_lock:
            set_permissions(path=backup_lock.name, gid=grp.getgrnam("omd").gr_gid, mode=0o660)
            os.rename(backup_lock.name, lock_file_path)
    except OSError as e:
        raise MKGeneralException(f'Failed to open lock file "{lock_file_path}": {e}')


@contextmanager
def acquire_single_lock(lock_file_path: Path) -> Generator[IO[bytes], None, None]:
    """Ensure only one "mkbackup" instance can run on each system at a time.
    We are using multiple locks:
    * one lock per site, which gets restored / backup
    * one global lock, in case mkbackup is executed as root in order to perform a system backup / restore

    Note:
        Please note that with 1.6.0p21 we have changed the path from
        /tmp/mkbackup.lock to /run/lock/mkbackup/mkbackup.lock. We had to move
        the lock file to a dedicated subdirectory without sticky bit to make it
        possible to write and lock files from different sites. The move from
        /tmp to /run/lock was just to have the lock file in a more specific
        place. See also:

        https://forum.checkmk.com/t/backup-schlagt-fehl/21630/7
        https://unix.stackexchange.com/a/503169
    """
    _ensure_lock_file(lock_file_path)

    with lock_file_path.open("ab") as backup_lock:
        try:
            fcntl.flock(backup_lock, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except OSError:
            raise MKGeneralException(
                "Failed to get the exclusive backup lock. "
                "Another backup/restore seems to be running."
            )

        # Ensure that the lock is not inherited to subprocessess
        try:
            cloexec_flag = fcntl.FD_CLOEXEC
        except AttributeError:
            cloexec_flag = 1

        fd = backup_lock.fileno()
        fcntl.fcntl(fd, fcntl.F_SETFD, fcntl.fcntl(fd, fcntl.F_GETFD) | cloexec_flag)
        yield backup_lock


def set_permissions(
    *, path: str, uid: int | None = None, gid: int | None = None, mode: int | None = None
) -> None:
    try:
        os.chown(path, uid if uid is not None else -1, gid if gid is not None else -1)
    except OSError as e:
        if e.errno == errno.EACCES:
            pass  # On CIFS mounts where "uid=0,forceuid,gid=1000,forcegid" mount options
            # are set, this is not possible. So skip over.
        elif e.errno == errno.EPERM:
            pass  # On NFS mounts where "" mount options are set, we get an
            # "Operation not permitted" error when trying to change e.g.
            # the group permission.
        else:
            raise

    try:
        if mode is not None:
            os.chmod(path, mode)
    except OSError as e:
        if e.errno == errno.EACCES:
            pass  # On CIFS mounts where "uid=0,forceuid,gid=1000,forcegid" mount options
            # are set, this is not possible. So skip over.
        elif e.errno == errno.EPERM:
            pass  # On NFS mounts where "" mount options are set, we get an
            # "Operation not permitted" error when trying to change e.g.
            # the group permission.
        else:
            raise


# TODO: Move to cmklib?
def makedirs(
    path: str, user: str | None = None, group: str | None = None, mode: int | None = None
) -> None:
    head, tail = os.path.split(path)
    if not tail:
        head, tail = os.path.split(head)

    if head and tail and not os.path.exists(head):
        try:
            makedirs(head, user, group, mode)
        except OSError as e:
            # be happy if someone already created the path
            if e.errno != errno.EEXIST:
                raise
        if tail == ".":  # xxx/newdir/. exists if xxx/newdir exists
            return
    makedir(path, user, group, mode)


# TODO: Move to cmklib?
def makedir(
    path: str, user: str | None = None, group: str | None = None, mode: int | None = None
) -> None:
    if os.path.exists(path):
        return
    os.mkdir(path)
    uid = pwd.getpwnam(user).pw_uid if user is not None else None
    gid = grp.getgrnam(group).gr_gid if group is not None else None
    set_permissions(path=path, uid=uid, gid=gid, mode=mode)


def file_checksum(path: Path) -> str:
    hash_md5 = md5(usedforsecurity=False)  # pylint: disable=unexpected-keyword-arg
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


# Using RSA directly to encrypt the whole backup is a bad idea. So we use the RSA
# public key to generate and encrypt a shared secret which is then used to encrypt
# the backup with AES.
#
# When encryption is active, this function uses the configured RSA public key to
# a) create a random secret key which is encrypted with the RSA public key
# b) the encrypted key is used written to the backup file
# c) the unencrypted random key is used as AES key for encrypting the backup stream
class MKBackupStream:
    def __init__(
        self, stream: IO[bytes], is_alive: Callable[[], bool], key_ident: str | None, state: State
    ) -> None:
        self._stream = stream
        self._is_alive = is_alive
        self._cipher: CbcMode | None = None
        self._key_ident = key_ident

        self._last_state_update = time.time()
        self._last_bps: float | None = None
        self._bytes_copied = 0
        self._next_chunk: bytes | None = None
        self._state = state

        # The iv is an initialization vector for the CBC mode of operation. It
        # needs to be unique per key per message. Normally, it's sent alongside
        # the data in cleartext. Here, since the key is only ever used once,
        # you can use a known IV.
        self._iv = b"\x00" * AES.block_size

    def process(self) -> Iterator[bytes]:
        head = self._init_processing()
        if head is not None:
            yield head

        self._next_chunk = None

        while True:
            chunk, finished = self._read_chunk()
            self._bytes_copied += len(chunk)
            yield self._process_chunk(chunk)

            if finished and not self._is_alive():
                break  # end of stream reached

            self._update_state()

    def _init_processing(self) -> bytes | None:
        raise NotImplementedError()

    def _read_from_stream(self, size: int) -> bytes:
        try:
            return self._stream.read(size)
        except ValueError:
            if self._stream.closed:
                return b""  # handle EOF transparently
            raise

    def _read_chunk(self) -> tuple[bytes, bool]:
        raise NotImplementedError()

    def _process_chunk(self, chunk: bytes) -> bytes:
        raise NotImplementedError()

    def _update_state(self) -> None:
        timedif = time.time() - self._last_state_update
        if timedif >= 1:
            this_bps = float(self._bytes_copied) / timedif

            if self._last_bps is None:
                bps = this_bps  # initialize the value
            else:
                percentile, backlog_sec = 0.50, 10
                weight_per_sec = (1 - percentile) ** (1.0 / backlog_sec)
                weight = weight_per_sec**timedif
                bps = self._last_bps * weight + this_bps * (1 - weight)

            self._state.save(new_attrs={"bytes_per_second": bps})
            self._last_state_update, self._last_bps, self._bytes_copied = time.time(), bps, 0

    def _get_key_spec(self, key_id: bytes) -> dict[str, bytes]:
        keys = self._load_backup_keys()

        for key in keys.values():
            digest: bytes = load_certificate_pem(key["certificate"]).digest("md5")
            if key_id == digest:
                return key

        raise MKGeneralException("Failed to load the configured backup key: %s" % key_id.decode())

    # TODO: The return type is a bit questionable...
    def _load_backup_keys(self) -> dict[str, dict[str, bytes]]:
        path = Path(os.environ["OMD_ROOT"], "etc/check_mk/backup_keys.mk")

        variables: dict[str, dict[str, Any]] = {"keys": {}}
        if path.exists():
            exec(path.read_text(), variables, variables)
        # TODO: Verify value of keys.
        return variables["keys"]


class BackupStream(MKBackupStream):
    def _init_processing(self) -> bytes | None:
        if self._key_ident is None:
            return None

        secret_key, encrypted_secret_key = self._derive_key(
            self._get_encryption_public_key(self._key_ident.encode("utf-8")), 32
        )
        cipher = AES.new(secret_key, AES.MODE_CBC, self._iv)
        assert isinstance(cipher, CbcMode)
        self._cipher = cipher

        # Write out a file version marker and  the encrypted secret key, preceded by
        # a length indication. All separated by \0.
        # Version 1: Encrypted secret key written with pubkey.encrypt(). Worked with
        #            early versions of 1.4 until moving from PyCryto to PyCryptodome
        # Version 2: Use PKCS1_OAEP for encrypting the encrypted_secret_key.
        return b"%d\0%d\0%s\0" % (2, len(encrypted_secret_key), encrypted_secret_key)

    def _read_chunk(self) -> tuple[bytes, bool]:
        finished = False
        if self._key_ident is not None:
            chunk = self._read_from_stream(1024 * AES.block_size)

            # Detect end of file and add padding to fill up to block size
            if chunk == b"" or len(chunk) % AES.block_size != 0:
                padding_length = (AES.block_size - len(chunk) % AES.block_size) or AES.block_size
                chunk += padding_length * bytes((padding_length,))
                finished = True
        else:
            chunk = self._read_from_stream(1024 * 1024)
            if chunk == b"":
                finished = True

        return chunk, finished

    def _process_chunk(self, chunk: bytes) -> bytes:
        if self._key_ident is not None:
            assert self._cipher is not None
            return self._cipher.encrypt(chunk)
        return chunk

    def _get_encryption_public_key(self, key_id: bytes) -> RSA.RsaKey:
        key = self._get_key_spec(key_id)

        # First extract the public key part from the certificate
        cert = load_certificate_pem(key["certificate"])
        pub: crypto.PKey = cert.get_pubkey()
        pub_pem = dump_publickey_pem(pub)

        # Now constuct the public key object
        return RSA.importKey(pub_pem)

    # logic from http://stackoverflow.com/questions/6309958/encrypting-a-file-with-rsa-in-python
    # Since our packages moved from PyCrypto to PyCryptodome we need to change this to use PKCS1_OAEP.
    def _derive_key(self, pubkey, key_length):
        secret_key = os.urandom(key_length)

        # Encrypt the secret key with the RSA public key
        cipher_rsa = PKCS1_OAEP.new(pubkey)
        encrypted_secret_key = cipher_rsa.encrypt(secret_key)

        return secret_key, encrypted_secret_key


class RestoreStream(MKBackupStream):
    def _init_processing(self) -> bytes | None:
        if self._key_ident is None:
            return None

        file_version, encrypted_secret_key = self._read_encrypted_secret_key()
        secret_key = self._decrypt_secret_key(
            file_version, encrypted_secret_key, self._key_ident.encode("utf-8")
        )
        cipher = AES.new(secret_key, AES.MODE_CBC, self._iv)
        assert isinstance(cipher, CbcMode)
        self._cipher = cipher
        return None

    def _read_chunk(self) -> tuple[bytes, bool]:
        if self._key_ident is None:
            # process unencrypted backup
            chunk = self._read_from_stream(1024 * 1024)
            return chunk, chunk == b""

        assert self._cipher is not None
        this_chunk = self._cipher.decrypt(self._read_from_stream(1024 * AES.block_size))

        if self._next_chunk is None:
            # First chunk. Only store for next loop
            self._next_chunk = this_chunk
            return b"", False

        if len(this_chunk) == 0:
            # Processing last chunk. Stip off padding.
            pad = self._next_chunk[-1]
            chunk = self._next_chunk[:-pad]
            return chunk, True

        # Processing regular chunk
        chunk = self._next_chunk
        self._next_chunk = this_chunk
        return chunk, False

    def _process_chunk(self, chunk: bytes) -> bytes:
        return chunk

    def _read_encrypted_secret_key(self) -> tuple[bytes, bytes]:
        def read_field() -> bytes:
            buf = b""
            while True:
                c = self._stream.read(1)
                if c == b"\0":
                    break
                buf += c
            return buf

        file_version = read_field()
        if file_version not in [b"1", b"2"]:
            raise MKGeneralException(
                "Failed to process backup file (invalid version %r)" % file_version
            )

        try:
            key_len = int(read_field())
        except ValueError:
            raise MKGeneralException("Failed to parse the encrypted backup file (key length)")

        if int(key_len) > 256:
            raise MKGeneralException("Failed to process backup file (invalid key length)")

        encrypted_secret_key = self._stream.read(int(key_len))

        if self._stream.read(1) != b"\0":
            raise MKGeneralException("Failed to parse the encrypted backup file (header broken)")

        return file_version, encrypted_secret_key

    def _get_encryption_private_key(self, key_id: bytes) -> RSA.RsaKey:
        key = self._get_key_spec(key_id)

        try:
            passphrase = os.environ["MKBACKUP_PASSPHRASE"]
        except KeyError:
            raise MKGeneralException(
                "Failed to get passphrase for decryption the backup. "
                "It needs to be given as environment variable "
                '"MKBACKUP_PASSPHRASE".'
            )

        # First decrypt the private key using PyOpenSSL (was unable to archieve
        # this with RSA.importKey(). :-(
        pkey = load_privatekey_pem(key["private_key"], passphrase.encode("utf-8"))
        priv_pem = dump_privatekey_pem(pkey)

        try:
            return RSA.importKey(priv_pem)
        except (ValueError, IndexError, TypeError):
            if opt_debug:
                raise
            raise MKGeneralException("Failed to load private key (wrong passphrase?)")

    def _decrypt_secret_key(
        self, file_version: bytes, encrypted_secret_key: bytes, key_id: bytes
    ) -> bytes:
        private_key = self._get_encryption_private_key(key_id)

        if file_version == b"1":
            raise MKGeneralException(
                "You can not restore this backup using your current Check_MK "
                "version. You need to use a Check_MK 1.4 version that has "
                "been released before 2017-03-24. The last compatible "
                "release is 1.4.0b4."
            )
        cipher_rsa = PKCS1_OAEP.new(private_key)
        return cipher_rsa.decrypt(encrypted_secret_key)


#########################################
# Schemas to share with CMA Backup tool #
# DO NOT CHANGE!                        #
#########################################

TargetId = NewType("TargetId", str)


class Config(TypedDict):
    targets: dict[str, Any]
    jobs: dict[str, Any]


class ScheduleConfig(TypedDict):
    disabled: bool
    period: str
    timeofday: Sequence[tuple[int, int]]


class JobConfig(TypedDict):
    encrypt: str | None
    target: TargetId
    compress: bool
    schedule: ScheduleConfig | None
    no_history: bool
    without_sites: bool


class BackupInfo(TypedDict):
    config: JobConfig
    files: Sequence[tuple[str, int, str]]
    finished: float
    hostname: str
    job_id: str
    site_id: str
    site_version: str
    size: int
    type: str
    backup_id: str | None


#########################################################
# Lock files are shared with cma backup tool            #
# We want to avoid running two backups at the same time #
#########################################################


def local_lock_file_path(site_name: str) -> Path:
    return mkbackup_lock_dir / Path(f"mkbackup-{site_name}.lock")


def get_needed_lock_files(opts: Mapping[str, str], job_config: JobConfig) -> Sequence[Path]:
    site_id = current_site_id()
    return [local_lock_file_path(site_id)]


######################
# End of shared code #
######################


@dataclass
class Job:
    config: JobConfig
    local_id: str
    id: str


@dataclass(frozen=True)
class BackupTarget:
    config: Config
    job: Job

    @property
    def base_path(self) -> Path:
        return target_path(self.job.config["target"], self.config) / f"{self.job.id}-incomplete"

    @property
    def target_id(self) -> str:
        return self.job.config["target"]

    @property
    def completed_path(self) -> Path:
        return target_path(self.job.config["target"], self.config) / f"{self.job.id}-complete"


@dataclass(frozen=True)
class RestoreTarget:
    target_id: TargetId
    backup_id: str
    config: Config

    @property
    def base_path(self) -> Path:
        return target_path(self.target_id, self.config) / self.backup_id


Target = BackupTarget | RestoreTarget


def backup_state(job: Job) -> State:
    path = Path(os.environ["OMD_ROOT"]) / "var" / "check_mk" / "backup"
    name = f"{job.local_id}.state"
    return State(path / name)


def restore_state() -> State:
    path = Path("/tmp")
    name = f"restore-{current_site_id()}.state"
    return State(path / name)


def current_site_id() -> str:
    return os.environ["OMD_SITE"]


def site_version(site_id: str) -> str:
    linkpath = os.readlink("/omd/sites/%s/version" % site_id)
    return linkpath.split("/")[-1]


def system_config_path() -> Path:
    return Path("/etc/cma/backup.conf")


def site_config_path() -> Path:
    return Path(os.environ["OMD_ROOT"]) / "etc" / "check_mk" / "backup.mk"


# Wenn als root ausgeführt:
# - System-Konfiguration laden
# Wenn als Site-User ausgeführt:
# - TargetIds aus System-Konfiguration laden
# - Site-Konfiguration laden


def load_config() -> Config:
    def load_file(path: Path) -> Any:
        with path.open("r", encoding="utf-8") as f:
            return ast.literal_eval(f.read())

    config = load_file(site_config_path())

    try:
        system_targets = load_file(system_config_path())["targets"]

        # only load non conflicting targets
        for target_ident, target_config in system_targets.items():
            if target_ident not in config["targets"]:
                config["targets"][target_ident] = target_config

    except OSError:
        # Not existing system wide config is OK. In this case there
        # are only backup targets from site config available.
        pass

    return config


# TODO: Duplicate code with htdocs/backup.py
def load_backup_info(path: str) -> BackupInfo:
    with open(path) as f:
        info = json.load(f)

    # Load the backup_id from the second right path component. This is the
    # base directory of the mkbackup.info file. The user might have moved
    # the directory, e.g. for having multiple backups. Allow that.
    # Maybe we need to changed this later when we allow multiple generations
    # of backups.
    info["backup_id"] = os.path.basename(os.path.dirname(path))

    return info


def save_backup_info(info: BackupInfo, target: Target) -> None:
    with open(backup_info_path(target), "w") as f:
        json.dump(info, f, sort_keys=True, indent=4, separators=(",", ": "))


def create_backup_info(target: BackupTarget, job: Job) -> BackupInfo:
    files = get_files_for_backup_info(target, job)

    info: BackupInfo = {
        "type": "Check_MK",
        "job_id": job.local_id,
        "config": job.config,
        "hostname": hostname(),
        "files": files,
        "finished": time.time(),
        "size": sum(f[1] for f in files),
        "site_id": current_site_id(),
        "site_version": site_version(current_site_id()),
        "backup_id": None,
    }

    return info


def get_files_for_backup_info(target: BackupTarget, job: Job) -> list[tuple[str, int, str]]:
    backup_path = target.base_path
    dir_entries: list[str] = os.listdir(backup_path)
    files = []
    for f in sorted(dir_entries):
        files.append((f, os.path.getsize(backup_path / f), file_checksum(backup_path / f)))
    return files


#   List: Alle Backups auflisten
#       Als Site-Nutzer sieht man nur die Site-Backups (auch die, die
#       durch die Systembackups erstellt wurden)
#   - Job-ID
#
#   Beispielbefehle:
#     # listet alle Backups auf die man sehen darf
#     mkbackup list nfs
#
#     # listet alle Backups auf die man sehen darf die zu diesem Job gehören
#     mkbackup list nfs --job=xxx
#
#   Restore:
#   - Job-ID
#   - Backup-ID
#     - Als Site-Nutzer muss man die Backup-ID eines Site-Backups angeben
#
#   Beispielbefehle:
#     # listet alle Backups auf die man sehen darf
#     mkbackup restore nfs backup-id-20
#
#   Show: Zeigt Metainfos zu einem Backup an
#   - Job-ID
#   - Backup-ID
#
#   Beispielbefehle:
#     mkbackup show nfs backup-id-20


class Arg(NamedTuple):
    id: str
    description: str


class Opt(NamedTuple):
    description: str


class Mode(NamedTuple):
    description: str
    args: list[Arg]
    opts: dict[str, Opt]
    runner: Callable[[list[str], dict[str, str], Config], None]


modes = {
    "backup": Mode(
        description=(
            "Starts creating a new backup. When executed as Check_MK site user, a backup of the "
            "current site is executed to the target of the given backup job."
        ),
        args=[
            Arg(
                id="Job-ID",
                description="The ID of the backup job to work with",
            ),
        ],
        opts={
            "background": Opt(description="Fork and execute the program in the background."),
        },
        runner=lambda args, opts, config: mode_backup(args[0], opts=opts, config=config),
    ),
    "restore": Mode(
        description=(
            "Starts the restore of a backup. In case you want to restore an encrypted backup, "
            "you have to provide the passphrase of the used backup key via the environment "
            "variable 'MKBACKUP_PASSPHRASE'. For example: MKBACKUP_PASSPHRASE='secret' mkbackup "
            "restore ARGS."
        ),
        args=[
            Arg(
                id="Target-ID",
                description="The ID of the backup target to work with",
            ),
            Arg(
                id="Backup-ID",
                description="The ID of the backup to restore",
            ),
        ],
        opts={
            "background": Opt(description="Fork and execute the program in the background."),
            "no-verify": Opt(
                description="Disable verification of the backup files to restore from."
            ),
            "no-reboot": Opt(description="Don't trigger a system reboot after succeeded restore."),
        },
        runner=lambda args, opts, config: mode_restore(args[0], args[1], opts=opts, config=config),
    ),
    "jobs": Mode(
        description="Lists all configured backup jobs of the current user context.",
        args=[],
        opts={},
        runner=lambda args, opts, config: mode_jobs(opts=opts, config=config),
    ),
    "targets": Mode(
        description="Lists all configured backup targets of the current user context.",
        args=[],
        opts={},
        runner=lambda args, opts, config: mode_targets(opts=opts, config=config),
    ),
    "list": Mode(
        description="Output the list of all backups found on the given backup target",
        args=[
            Arg(
                id="Target-ID",
                description="The ID of the backup target to work with",
            ),
        ],
        opts={},
        runner=lambda args, opts, config: mode_list(args[0], opts=opts, config=config),
    ),
}


def mode_backup(local_job_id: str, opts: dict[str, str], config: Config) -> None:
    job = load_job(local_job_id, config)
    target = BackupTarget(config, job)
    state = backup_state(job)

    with ExitStack() as stack:
        for cm in [
            acquire_single_lock(lock_file_path)
            for lock_file_path in get_needed_lock_files(opts, job.config)
        ]:
            stack.enter_context(cm)
        target_ident = job.config["target"]
        verify_target_is_ready(target_ident, config)

        init_new_run(state)
        save_next_run(job, state)

        if "background" in opts:
            daemon.daemonize()
            state.save({"pid": os.getpid()})

        start_logging(state)
        log(f"--- Starting backup ({job.id} to {target_ident}) ---")

        success = False
        try:
            cleanup_previous_incomplete_backup(target)

            state.save(
                {
                    "state": "running",
                },
            )

            do_site_backup(job, target, state)
            complete_backup(job, target, state)
            success = True

        except MKGeneralException as e:
            sys.stderr.write("%s\n" % e)
            if opt_debug:
                raise

        except Exception:
            if opt_debug:
                raise
            sys.stderr.write("An exception occured:\n")
            sys.stderr.write(traceback.format_exc())

        finally:
            stop_logging()
            state.save(
                {
                    "state": "finished",
                    "finished": time.time(),
                    "success": success,
                },
            )


def do_site_backup(
    job: Job,
    target: BackupTarget,
    state: State,
    site: str | None = None,
    try_stop: bool = True,
) -> None:  # pylint: disable=too-many-branches
    cmd = ["omd", "backup"]

    if not job.config["compress"]:
        cmd.append("--no-compression")

    if job.config.get("no_history", False):
        cmd.append("--no-past")

    site = current_site_id()
    cmd.append("-")

    backup_path = site_backup_archive_path(site, target, job.config)

    # Create missing directories. Ensure group permissions and mode.
    makedirs(os.path.dirname(backup_path), group="omd", mode=0o775)

    verbose("Command: %s" % " ".join(cmd))
    with subprocess.Popen(
        cmd,
        close_fds=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        stdin=subprocess.DEVNULL,
    ) as p:
        assert p.stdout is not None
        assert p.stderr is not None

        with open(backup_path, "wb") as backup_file:
            s = BackupStream(
                stream=p.stdout,
                is_alive=lambda: p.poll() is None,
                key_ident=job.config["encrypt"],
                state=state,
            )
            for chunk in s.process():
                backup_file.write(chunk)

        err = p.stderr.read().decode()

    if p.returncode != 0:
        raise MKGeneralException("Site backup failed: %s" % err)


# Returns the base path for the backup to work with. In backup mode, this is
# the directory of the target+job. In restore mode it is the target+backup path.
def backup_info_path(target: Target) -> str:
    return f"{target.base_path}/mkbackup.info"


def site_backup_archive_path(site_id: str, target: Target, job_config: JobConfig) -> Path:
    return target.base_path / f"site-{site_id}{archive_suffix(job_config)}"


def archive_suffix(config: JobConfig) -> str:
    suffix = ".tar"
    if config["compress"]:
        suffix += ".gz"
    if config["encrypt"]:
        suffix += ".enc"
    return suffix


def needed_backup_archive_files(info: BackupInfo) -> Sequence[str]:
    # Care about restore from a backup made in a site with another site_id
    site = info.get("site_id", current_site_id())
    needed_files = ["site-%s" % site]

    return [f + archive_suffix(info["config"]) for f in needed_files]


def target_cfg(target_ident: str, config: Config) -> Mapping[str, Any]:
    return config["targets"][target_ident]


def target_path(target_ident: str, config: Config) -> Path:
    cfg = target_cfg(target_ident, config)
    if cfg["remote"][0] != "local":
        raise NotImplementedError()

    return Path(cfg["remote"][1]["path"])


# TODO: Duplicate code with htdocs/backup.py
def verify_target_is_ready(target_ident: str, config: Config) -> None:
    try:
        cfg = target_cfg(target_ident, config)
    except KeyError:
        raise MKGeneralException('The backup target "%s" does not exist.' % target_ident)

    if cfg["remote"][0] != "local":
        raise NotImplementedError()

    if cfg["remote"][1]["is_mountpoint"] and not os.path.ismount(cfg["remote"][1]["path"]):
        raise MKGeneralException(
            "The backup target path is configured to be a mountpoint, but nothing is mounted."
        )


def verify_backup_exists(target: Target) -> None:
    if not os.path.exists(target.base_path) or not os.path.exists(backup_info_path(target)):
        raise MKGeneralException(
            f'This backup does not exist (Use "mkbackup list {target.target_id}" to '
            "show a list of available backups)."
        )


def verify_backup_consistency(info: BackupInfo, target: Target) -> None:
    log("Verifying backup consistency")
    needed_files = needed_backup_archive_files(info)
    optional_files = [entry[0] for entry in info["files"] if entry[0] not in needed_files]

    verify_backup_files(info, needed_files, needed=True, target=target)
    verify_backup_files(info, optional_files, needed=False, target=target)


def verify_backup_files(
    info: BackupInfo, files: Sequence[str], needed: bool, target: Target
) -> None:
    for archive_file in files:
        size, checksum = None, None
        for entry in info["files"]:
            if entry[0] == archive_file:
                size, checksum = entry[1:]
                break

        if size is None:
            if needed:
                raise MKGeneralException(
                    "The backup is missing the needed archive %s." % archive_file
                )
            continue  # missing optional files are OK

        archive_path = target.base_path / archive_file
        this_checksum = file_checksum(archive_path)
        if this_checksum != checksum:
            raise MKGeneralException(
                "The backup seems to be damaged and can not be restored. "
                "The checksum of the archive %s is wrong (got %s but "
                "expected %s)." % (archive_path, this_checksum, checksum)
            )


def complete_backup(job: Job, target: BackupTarget, state: State) -> None:
    info = create_backup_info(target, job)
    save_backup_info(info, target)

    state.save(
        {
            "size": info["size"],
        },
    )

    verify_backup_consistency(info, target=target)

    # Now we can be sure this new backup is a good one. Remove eventual old
    # backup and move from "incomplete" to "complete".

    if os.path.exists(target.completed_path):
        log("Cleaning up previously completed backup")
        shutil.rmtree(target.completed_path)

    os.rename(target.base_path, target.completed_path)

    state_content = state.load()
    duration = time.time() - state_content["started"]

    log(
        "--- Backup completed (Duration: %s, Size: %s, IO: %s/s) ---"
        % (
            render.timespan(duration),
            render.fmt_bytes(info["size"]),
            render.fmt_bytes(state_content["bytes_per_second"]),
        )
    )


def cleanup_previous_incomplete_backup(target: BackupTarget) -> None:
    if os.path.exists(target.base_path):
        log("Found previous incomplete backup. Cleaning up those files.")
        try:
            shutil.rmtree(target.base_path)
        except OSError as e:
            if e.errno == errno.EACCES:
                raise MKGeneralException("Failed to write the backup directory: %s" % e)
            raise


def load_job(local_job_id: str, config: Config) -> Job:
    g_job_id = globalize_job_id(local_job_id)

    if local_job_id not in config["jobs"]:
        raise MKGeneralException("This backup job does not exist.")

    job = Job(config=config["jobs"][local_job_id], local_id=local_job_id, id=g_job_id)
    return job


def globalize_job_id(local_job_id: str) -> str:
    parts = ["Check_MK", hostname(), current_site_id(), local_job_id]
    return "-".join(p.replace("-", "+") for p in parts)


def init_new_run(state: State) -> None:
    state.save(
        {
            "state": "started",
            "pid": os.getpid(),
            "started": time.time(),
            "output": "",
            "bytes_per_second": 0,
        },
        update=False,
    )


def save_next_run(job: Job, state: State) -> None:
    schedule_cfg = job.config["schedule"]
    if not schedule_cfg:
        next_schedule: str | float | None = None

    elif schedule_cfg["disabled"]:
        next_schedule = "disabled"

    else:
        # find the next time of all configured times
        times = []
        for timespec in schedule_cfg["timeofday"]:
            times.append(schedule.next_scheduled_time(schedule_cfg["period"], timespec))
        next_schedule = min(times)

    state.save({"next_schedule": next_schedule})


def cleanup_backup_job_states() -> None:
    path = "%s/var/check_mk/backup" % os.environ["OMD_ROOT"]

    for f in glob.glob("%s/*.state" % path):
        if os.path.basename(f) != "restore.state" and not os.path.basename(f).startswith(
            "restore-"
        ):
            os.unlink(f)


def mode_restore(target_id: str, backup_id: str, opts: dict[str, str], config: Config) -> None:
    target = RestoreTarget(TargetId(target_id), backup_id, config)
    info = load_backup_info(backup_info_path(target))
    state = restore_state()

    with ExitStack() as stack:
        for cm in [
            acquire_single_lock(lock_file_path)
            for lock_file_path in get_needed_lock_files(opts, info["config"])
        ]:
            stack.enter_context(cm)

        verify_target_is_ready(target_id, config)
        verify_backup_exists(target)

        if "no-verify" not in opts:
            verify_backup_consistency(info, target)

        init_new_run(state)

        if "background" in opts:
            daemon.daemonize()
            state.save({"pid": os.getpid()})

        start_logging(state)

        log("--- Starting restore (%s) ---" % backup_id)

        success = False
        try:
            state.save(
                {
                    "state": "running",
                },
            )

            do_site_restore(info, target, state)
            complete_restore(state)
            success = True

        except MKGeneralException as e:
            sys.stderr.write("%s\n" % e)
            if opt_debug:
                raise

        except Exception:
            if opt_debug:
                raise
            sys.stderr.write("An exception occured:\n")
            sys.stderr.write(traceback.format_exc())

        finally:
            stop_logging()
            state.save(
                {
                    "state": "finished",
                    "finished": time.time(),
                    "success": success,
                },
            )


def do_site_restore(
    info: BackupInfo,
    target: RestoreTarget,
    state: State,
) -> None:
    cmd = ["omd", "restore", "--kill"]

    # TODO use backup from different site
    site = info.get("site_id", current_site_id())

    cmd.append("-")

    backup_path = site_backup_archive_path(site, target, info["config"])

    with subprocess.Popen(cmd, close_fds=True, stderr=subprocess.PIPE, stdin=subprocess.PIPE) as p:
        assert p.stdin is not None
        assert p.stderr is not None

        with open(backup_path, "rb") as backup_file:
            s = RestoreStream(
                stream=backup_file,
                is_alive=lambda: False,
                key_ident=info["config"]["encrypt"],
                state=state,
            )
            try:
                for chunk in s.process():
                    p.stdin.write(chunk)
            except OSError as e:
                log("Error while sending data to restore process: %s" % e)

        _stdout, stderr = p.communicate()

    if p.returncode:
        log(stderr.decode(encoding="utf-8", errors="strict"))
        raise MKGeneralException("Site restore failed")

    if subprocess.call(["omd", "start"]) != 0:
        raise MKGeneralException("Failed to start the site after restore")


def complete_restore(state: State) -> None:
    cleanup_backup_job_states()
    state_content = state.load()
    duration = time.time() - state_content["started"]
    log(
        "--- Restore completed (Duration: %s, IO: %s/s) ---"
        % (render.timespan(duration), render.fmt_bytes(state_content["bytes_per_second"]))
    )


def mode_list(target_id: str, opts: dict[str, str], config: Config) -> None:
    if target_id not in config["targets"]:
        raise MKGeneralException(
            "This backup target does not exist. Choose one of: %s"
            % ", ".join(config["targets"].keys())
        )

    verify_target_is_ready(target_id, config)

    fmt = "%-9s %-20s %-16s %52s\n"
    fmt_detail = (" " * 30) + " %-20s %48s\n"
    sys.stdout.write(fmt % ("Type", "Job", "Details", ""))
    sys.stdout.write("%s\n" % ("-" * 100))
    for path in sorted(glob.glob("%s/*/mkbackup.info" % target_path(target_id, config))):
        info = load_backup_info(path)
        from_info = info["hostname"]
        if "site_id" in info:
            from_info += " (Site: %s)" % info["site_id"]
        sys.stdout.write(fmt % (info["type"], info["job_id"], "Backup-ID:", info["backup_id"]))

        sys.stdout.write(fmt_detail % ("From:", from_info))
        sys.stdout.write(fmt_detail % ("Finished:", render.date_and_time(info["finished"])))
        sys.stdout.write(fmt_detail % ("Size:", render.fmt_bytes(info["size"])))
        if info["config"]["encrypt"] is not None:
            sys.stdout.write(fmt_detail % ("Encrypted:", info["config"]["encrypt"]))
        else:
            sys.stdout.write(fmt_detail % ("Encrypted:", "No"))
        sys.stdout.write("\n")
    sys.stdout.write("\n")


def mode_jobs(opts: Mapping[str, str], config: Config) -> None:
    fmt = "%-29s %-30s\n"
    sys.stdout.write(fmt % ("Job-ID", "Title"))
    sys.stdout.write("%s\n" % ("-" * 60))
    for job_id, job_cfg in sorted(config["jobs"].items(), key=lambda x_y: x_y[0]):
        sys.stdout.write(fmt % (job_id, job_cfg["title"]))


def mode_targets(opts: dict[str, str], config: Config) -> None:
    fmt = "%-29s %-30s\n"
    sys.stdout.write(fmt % ("Target-ID", "Title"))
    sys.stdout.write("%s\n" % ("-" * 60))
    for job_id, job_cfg in sorted(config["targets"].items(), key=lambda x_y1: x_y1[0]):
        sys.stdout.write(fmt % (job_id, job_cfg["title"]))


def usage(error: str | None = None) -> NoReturn:
    if error:
        sys.stderr.write("ERROR: %s\n" % error)
    sys.stdout.write("Usage: mkbackup [OPTIONS] MODE [MODE_ARGUMENTS...] [MODE_OPTIONS...]\n")
    sys.stdout.write("\n")
    sys.stdout.write("OPTIONS:\n")
    sys.stdout.write("\n")
    sys.stdout.write("    --verbose     Enable verbose output, twice for more details\n")
    sys.stdout.write("    --debug       Let Python exceptions come through\n")
    sys.stdout.write("    --version     Print the version of the program\n")
    sys.stdout.write("\n")
    sys.stdout.write("MODES:\n")
    sys.stdout.write("\n")

    for mode_name, mode in sorted(modes.items()):
        mode_indent = " " * 18
        wrapped_descr = textwrap.fill(
            mode.description,
            width=82,
            initial_indent="    %-13s " % mode_name,
            subsequent_indent=mode_indent,
        )
        sys.stdout.write(wrapped_descr + "\n")
        sys.stdout.write("\n")
        if mode.args:
            sys.stdout.write("%sMODE ARGUMENTS:\n" % mode_indent)
            sys.stdout.write("\n")
            for arg in mode.args:
                sys.stdout.write("%s  %-10s %s\n" % (mode_indent, arg.id, arg.description))
            sys.stdout.write("\n")

        opts = mode_options(mode)
        if opts:
            sys.stdout.write("%sMODE OPTIONS:\n" % mode_indent)
            sys.stdout.write("\n")

            for opt_id, opt in sorted(opts.items(), key=lambda k_v: k_v[0]):
                sys.stdout.write("%s  --%-13s %s\n" % (mode_indent, opt_id, opt.description))
            sys.stdout.write("\n")

    sys.stdout.write("\n")
    sys.exit(3)


def mode_options(mode: Mode) -> dict[str, Opt]:
    opts = {}
    opts.update(mode.opts)
    return opts


def interrupt_handler(signum: int, frame: Any) -> NoReturn:
    raise MKTerminate("Caught signal: %d" % signum)


def register_signal_handlers() -> None:
    signal.signal(signal.SIGTERM, interrupt_handler)


opt_verbose = 0
opt_debug = False


def parse_arguments():
    global opt_verbose, opt_debug
    short_options = "h"
    long_options = ["help", "version", "verbose", "debug"]

    try:
        opts, args = getopt.getopt(sys.argv[1:], short_options, long_options)
    except getopt.GetoptError as e:
        usage("%s" % e)

    for o, _unused_a in opts:
        if o in ["-h", "--help"]:
            usage()
        elif o == "--version":
            sys.stdout.write("mkbackup %s\n" % VERSION)
            sys.exit(0)
        elif o == "--verbose":
            opt_verbose += 1
        elif o == "--debug":
            opt_debug = True

    try:
        mode_name = args.pop(0)
    except IndexError:
        usage("Missing operation mode")

    try:
        mode = modes[mode_name]
    except KeyError:
        usage("Invalid operation mode")

    # Load the mode specific options
    try:
        mode_opts, mode_args = getopt.getopt(args, "", list(mode_options(mode).keys()))
    except getopt.GetoptError as e:
        usage("%s" % e)

    # Validate arguments
    if len(mode_args) != len(mode.args):
        usage("Invalid number of arguments for this mode")

    return mode, mode_args, mode_opts, opts


def main() -> None:
    register_signal_handlers()
    try:
        config = load_config()
    except OSError:
        if opt_debug:
            raise
        raise MKGeneralException("mkbackup is not configured yet.")
    mode, mode_args, mode_opts, opts = parse_arguments()
    try:
        current_site_id()
    except KeyError:
        raise MKGeneralException("Running outside of site context.")
    # Ensure the backup lock path exists and has correct permissions
    makedirs(str(mkbackup_lock_dir), group="omd", mode=0o770)
    opt_dict = {k.lstrip("-"): v for k, v in opts + mode_opts}
    mode.runner(mode_args, opt_dict, config)


if __name__ == "__main__":
    try:
        main()
    except MKTerminate as exc:
        sys.stderr.write("%s\n" % exc)
        sys.exit(1)

    except KeyboardInterrupt:
        sys.stderr.write("Terminated.\n")
        sys.exit(0)

    except MKGeneralException as exc:
        sys.stderr.write("%s\n" % exc)
        if opt_debug:
            raise
        sys.exit(3)
