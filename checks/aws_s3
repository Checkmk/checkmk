#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (C) 2019 tribe29 GmbH - License: GNU General Public License v2
# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and
# conditions defined in the file COPYING, which is part of this source code package.

# NOTE: Careful when replacing the *-import below with a more specific import. This can cause
# problems because it might remove variables needed for accessing discovery rulesets.
from cmk.base.check_legacy_includes.aws import *  # pylint: disable=wildcard-import,unused-wildcard-import


def parse_aws_s3(info):  # pylint: disable=function-redefined
    parsed = {}
    for row in parse_aws(info):
        bucket = parsed.setdefault(row["Label"], {})
        try:
            bucket["LocationConstraint"] = row["LocationConstraint"]
        except KeyError:
            pass
        try:
            bucket["Tagging"] = row["Tagging"]
        except KeyError:
            pass
        storage_key, size_key = row["Id"].split("_")[-2:]
        inst = bucket.setdefault(size_key, {})
        try:
            # if the entry exists, the first value is the numerical value of the metric and the
            # second one is the period, which is None here since these are not statistics of type
            # "Sum"
            inst.setdefault(storage_key, row["Values"][0][0])
        except (IndexError, ValueError):
            pass
    return parsed


#   .--S3 objects----------------------------------------------------------.
#   |            ____ _____         _     _           _                    |
#   |           / ___|___ /    ___ | |__ (_) ___  ___| |_ ___              |
#   |           \___ \ |_ \   / _ \| '_ \| |/ _ \/ __| __/ __|             |
#   |            ___) |__) | | (_) | |_) | |  __/ (__| |_\__ \             |
#   |           |____/____/   \___/|_.__// |\___|\___|\__|___/             |
#   |                                  |__/                                |
#   '----------------------------------------------------------------------'


@get_parsed_item_data
def check_aws_s3_objects(item, params, metrics):
    bucket_sizes = metrics["BucketSizeBytes"]
    storage_infos = []
    for storage_type, value in bucket_sizes.items():
        storage_infos.append("%s: %s" % (storage_type, get_bytes_human_readable(value)))
    sum_size = sum(bucket_sizes.values())
    yield check_levels(
        sum_size,
        "aws_bucket_size",
        params.get("bucket_size_levels", (None, None)),
        human_readable_func=get_bytes_human_readable,
        infoname="Bucket size",
    )
    if storage_infos:
        yield 0, ", ".join(storage_infos)

    num_objects = sum(metrics["NumberOfObjects"].values())
    yield 0, "Number of objects: %s" % int(num_objects), [("aws_num_objects", num_objects)]

    location = metrics.get("LocationConstraint")
    if location:
        yield 0, "Location: %s" % location

    tag_infos = []
    for tag in metrics.get("Tagging", {}):
        tag_infos.append("%s: %s" % (tag["Key"], tag["Value"]))
    if tag_infos:
        yield 0, "[Tags] %s" % ", ".join(tag_infos)


check_info["aws_s3"] = {
    "parse_function": parse_aws_s3,
    "inventory_function": lambda p: inventory_aws_generic(
        p, ["BucketSizeBytes", "NumberOfObjects"]
    ),
    "check_function": check_aws_s3_objects,
    "service_description": "AWS/S3 Objects %s",
    "has_perfdata": True,
    "group": "aws_s3_buckets_objects",
}

# .
#   .--summary-------------------------------------------------------------.
#   |                                                                      |
#   |           ___ _   _ _ __ ___  _ __ ___   __ _ _ __ _   _             |
#   |          / __| | | | '_ ` _ \| '_ ` _ \ / _` | '__| | | |            |
#   |          \__ \ |_| | | | | | | | | | | | (_| | |  | |_| |            |
#   |          |___/\__,_|_| |_| |_|_| |_| |_|\__,_|_|   \__, |            |
#   |                                                    |___/             |
#   '----------------------------------------------------------------------'


def check_aws_s3_summary(item, params, parsed):
    sum_size = 0
    largest_bucket = None
    largest_bucket_size = 0
    for bucket_name, bucket in parsed.items():
        bucket_size = sum(bucket["BucketSizeBytes"].values())
        sum_size += bucket_size
        if bucket_size >= largest_bucket_size:
            largest_bucket = bucket_name
            largest_bucket_size = bucket_size
    yield check_levels(
        sum_size,
        "aws_bucket_size",
        params.get("bucket_size_levels", (None, None)),
        human_readable_func=get_bytes_human_readable,
        infoname="Total size",
    )

    if largest_bucket:
        yield 0, "Largest bucket: %s (%s)" % (
            largest_bucket,
            get_bytes_human_readable(largest_bucket_size),
        ), [("aws_largest_bucket_size", largest_bucket_size)]


check_info["aws_s3.summary"] = {
    "inventory_function": discover_single,
    "check_function": check_aws_s3_summary,
    "service_description": "AWS/S3 Summary",
    "has_perfdata": True,
    "group": "aws_s3_buckets",
}
