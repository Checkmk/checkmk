#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (C) 2019 tribe29 GmbH - License: GNU General Public License v2
# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and
# conditions defined in the file COPYING, which is part of this source code package.

# Example output:
# <<<fileinfo:sep(124)>>>
# 12968175080
# M:\check_mk.ini|missing
# M:\check_mk.ini|1390|12968174867
# M:\check_mk_agent.cc|86277|12968174554
# M:\Makefile|1820|12964010975
# M:\check_mk_agent.exe|102912|12968174364
# M:\crash.cc|1672|12964010975
# M:\crash.exe|20024|12968154426

# <<<fileinfo:sep(124)>>>
# 12968175080
# FILENAME|not readable|12968154426

# Parameters
# "minsize" : ( 5000,  4000 ),  in bytes
# "maxsize" : ( 8000,  9000 ),  in bytes
# "minage"  : ( 600,  1200 ),  in seconds
# "maxage"  : ( 6000, 12000 ), in seconds

import collections
import typing
import re
import cmk.base.plugins.agent_based.utils.eval_regex as eval_regex

fileinfo_groups = []


def _get_field(row, index, data_type):
    try:
        return data_type(row[index])
    except (IndexError, ValueError, TypeError):
        return None


def _parse_backwards_compatible(info):
    for row in info[1:]:
        name = row[0]
        # endswith("No such file...") is needed to
        # support the very old solaris perl based version of fileinfo
        if not name or name.endswith("No such file or directory"):
            continue
        ambiguous = _get_field(row, 1, str)
        missing = ambiguous == 'missing'
        failed = ambiguous in ('not readable', '')
        size = _get_field(row, 1, int)
        time = _get_field(row, 2, int)

        yield name, missing, failed, size, time


def parse_fileinfo(info):
    if not info:
        return {}
    parsed = {'files': {}}

    parsed['reftime'] = _get_field(info[0], 0, int)

    FileinfoItem = collections.namedtuple("FileinfoItem", "name missing failed size time")

    # deal with old agents' output:
    if len(info) > 1 and _get_field(info[1], 0, str) != '[[[header]]]':
        for args in _parse_backwards_compatible(info):
            fi = FileinfoItem(*args)
            parsed['files'][fi.name] = fi
        return parsed

    if len(info) < 3:
        return parsed

    header = info[2]
    name_i = header.index('name') if 'name' in header else None
    size_i = header.index('size') if 'size' in header else None
    time_i = header.index('time') if 'time' in header else None
    stat_i = header.index('status') if 'status' in header else None

    for row in info[4:]:
        name = _get_field(row, name_i, str)
        stat = _get_field(row, stat_i, str)
        size = _get_field(row, size_i, int)
        time = _get_field(row, time_i, int)

        if name is None or stat is None:
            continue

        fi = FileinfoItem(name, ('missing' in stat), ('stat failed' in stat), size, time)
        parsed['files'][fi.name] = fi

    return parsed


def _match(name: str, pattern: str) -> typing.Union[bool, typing.Match]:
    return (regex(pattern[1:]).match(name) if pattern.startswith("~") else  #
            fnmatch.fnmatch(name, pattern))


def fileinfo_groups_get_group_name(group_patterns, filename, reftime):
    found_these_groups = {}
    for group_name, pattern in group_patterns:
        if isinstance(pattern, str):  # support old format
            inclusion, exclusion = pattern, ''
        else:
            inclusion, exclusion = pattern

        if _match(filename, exclusion):
            continue

        inclusion = (("~" + fileinfo_process_date(inclusion[1:], reftime))
                     if inclusion.startswith("~") else fileinfo_process_date(inclusion, reftime))

        incl_match = _match(filename, inclusion)
        if not incl_match:
            continue

        matches = []
        num_perc_s = 0
        if incl_match is not True:  # it's match object then!
            num_perc_s = group_name.count("%s")
            matches = [g and g or "" for g in incl_match.groups()]

            if len(matches) < num_perc_s:
                raise MKGeneralException(
                    "Invalid entry in inventory_fileinfo_groups: "
                    "group name '%s' contains %d times '%%s', but regular expression "
                    "'%s' contains only %d subexpression(s)." %
                    (group_name, num_perc_s, inclusion, len(matches)))

        if matches:
            for nr, group in enumerate(matches):
                inclusion = eval_regex.instantiate_regex_pattern_once(inclusion, group)
                group_name = group_name.replace("%%%d" % (nr + 1), group)

            this_group_name = group_name % tuple(matches[:num_perc_s])
            this_pattern = ("~%s" % inclusion, exclusion)

        else:
            this_group_name = group_name
            this_pattern = pattern

        found_these_groups.setdefault(this_group_name, set()).add(this_pattern)

    # Convert pattern containers to lists (sets are not possible in autochecks)
    # It is possible now. keep this to not break the "append" in the check function
    return dict([(k, list(v)) for k, v in found_these_groups.items()])


def inventory_fileinfo_common(parsed, case):
    inventory = []

    reftime = parsed.get('reftime')
    if reftime is None:
        return inventory

    inventory_groups = host_extra_conf(host_name(), fileinfo_groups)

    for item in parsed['files'].values():
        found_groups = {}
        for group_patterns in inventory_groups:
            found_groups.update(fileinfo_groups_get_group_name(group_patterns, item.name, reftime))

        if not found_groups and case == 'single' and not item.missing:
            inventory.append((item.name, {}))

        elif found_groups and case == 'group':
            for group_name, patterns in found_groups.items():
                inventory.append((group_name, {"group_patterns": patterns}))

    return inventory


def fileinfo_process_date(pattern, reftime):
    for what, the_time in [("DATE", reftime), ("YESTERDAY", reftime - 86400)]:
        the_regex = r'((?:/|[A-Za-z]).*)\$%s:((?:%%\w.?){1,})\$(.*)' % what
        disect = re.match(the_regex, pattern)
        if disect:
            prefix = disect.group(1)
            datepattern = time.strftime(disect.group(2), time.localtime(the_time))
            postfix = disect.group(3)
            pattern = prefix + datepattern + postfix
            return pattern
    return pattern


def fileinfo_check_timeranges(params):
    ranges = params.get("timeofday")
    if ranges is None:
        return ""

    now = time.localtime()
    for range_spec in ranges:
        if fileinfo_in_timerange(now, *range_spec):
            return ""
    return "Out of relevant time of day"


def fileinfo_in_timerange(now, range_from, range_to):
    minutes_from = range_from[0] * 60 + range_from[1]
    minutes_to = range_to[0] * 60 + range_to[1]
    minutes_now = now.tm_hour * 60 + now.tm_min
    return minutes_now >= minutes_from and minutes_now < minutes_to


def check_fileinfo(item, params, parsed):

    reftime = parsed.get('reftime')
    if reftime is None:
        return 3, "Missing reference timestamp"

    outof_range_txt = fileinfo_check_timeranges(params)

    file_stat = parsed['files'].get(item)
    if file_stat is not None and not file_stat.missing:
        if file_stat.failed:
            return 1, "File stat failed"
        if file_stat.time is None:
            return 1, 'File stat time failed'
        age = reftime - file_stat.time
        check_definition = [
            ("Size", "size", file_stat.size, get_filesize_human_readable),
            ("Age", "age", age, get_age_human_readable),
        ]
        return _fileinfo_check_function(check_definition, params, outof_range_txt)
    else:
        if outof_range_txt:
            return 0, "File not found - %s" % outof_range_txt
        return params.get("state_missing", 3), "File not found"


def _filename_matches(filename, reftime, inclusion, exclusion):
    date_inclusion = ""
    inclusion_is_regex = False
    exclusion_is_regex = False

    if inclusion.startswith("~"):
        inclusion_is_regex = True
        inclusion = inclusion[1:]
    if exclusion.startswith("~"):
        exclusion_is_regex = True
        exclusion = exclusion[1:]

    inclusion_tmp = fileinfo_process_date(inclusion, reftime)
    if inclusion != inclusion_tmp:
        inclusion = inclusion_tmp
        date_inclusion = inclusion_tmp

    if inclusion_is_regex:
        incl_match = regex(inclusion).match(filename)
    else:
        incl_match = fnmatch.fnmatch(filename, inclusion)

    if exclusion_is_regex:
        excl_match = regex(exclusion).match(filename)
    else:
        excl_match = fnmatch.fnmatch(filename, exclusion)
    return incl_match and not excl_match, date_inclusion


def _update_minmax(new_value: float, current_minmax: typing.Optional[typing.Tuple[float, float]]):

    if not current_minmax:
        return new_value, new_value

    current_min, current_max = current_minmax

    return min(new_value, current_min), max(new_value, current_max)


def _check_individual_files(params, file_size, file_age):
    '''This function checks individual files against levels defined for the file group.
    This is done to generate information for the long output.'''
    states = []
    for key, value in [
        ("age_oldest", file_age),
        ("age_newest", file_age),
        ("size_smallest", file_size),
        ("size_largest", file_size),
    ]:
        levels = params.get("max" + key, (None, None)) + params.get("min" + key, (None, None))
        state, _text, _no_perf = check_levels(value, key, levels)
        states.append(state)

    overall_state = max(states)
    return overall_state, 'Age: %s, Size: %s%s' % (get_age_human_readable(file_age),
                                                   get_filesize_human_readable(file_size),
                                                   state_markers[overall_state])


def _define_fileinfo_group_check(files_matching, date_inclusion):
    size_smallest, size_largest = files_matching['size_minmax'] or (None, None)
    age_newest, age_oldest = files_matching['age_minmax'] or (None, None)
    return [
        ("Count", "count", files_matching['count_all'], saveint),
        ("Size", "size", files_matching['size_all'], get_filesize_human_readable),
        ("Largest size", "size_largest", size_largest, get_filesize_human_readable),
        ("Smallest size", "size_smallest", size_smallest, get_filesize_human_readable),
        ("Oldest age", "age_oldest", age_oldest, get_age_human_readable),
        ("Newest age", "age_newest", age_newest, get_age_human_readable),
        ("Date pattern", "date pattern", date_inclusion, str),
    ]


def _get_file_group_key(additional_rules, file_name):
    for file_expression, rules in additional_rules:
        if re.match(file_expression, file_name):
            return file_expression, rules

    return 'default', None


def check_fileinfo_groups(_item, params, parsed):

    all_files = parsed.get('files', {})
    outof_range_txt = fileinfo_check_timeranges(params)
    date_inclusion = None
    files_stat_failed = set()
    files_matching = {
        'default': {
            'count_all': 0,
            'size_all': 0,
            'size_minmax': None,
            'age_minmax': None,
            'rules': params,
            'file_text': {},
        }
    }

    skip_ok_files = params.get('shorten_multiline_output', False)

    reftime = parsed.get('reftime')
    if not reftime:
        yield 3, "Missing reference timestamp"
        return

    patterns = params.get('group_patterns')
    if not patterns:
        if all_files:
            yield 3, ("No group pattern found in autocheck. Please rediscover the services "
                      "of this host to get this fixed automatically")
            return
        default_files = files_matching['default']
        check_definition = _define_fileinfo_group_check(default_files, date_inclusion)
        yield from _fileinfo_check_function(
            check_definition,
            default_files['rules'],
            outof_range_txt,
        )
        return

    group_patterns = set((p, '') if isinstance(p, str) else p for p in patterns)

    additional_rules = params.get('additional_rules', [])

    # Start counting values on all files
    for file_stat in all_files.values():
        if file_stat.missing:
            continue

        for inclusion, exclusion in group_patterns:

            filename_matches, date_inclusion = _filename_matches(
                file_stat.name,
                reftime,
                inclusion,
                exclusion,
            )
            if not filename_matches:
                continue
            if file_stat.failed:
                files_stat_failed.add(file_stat.name)
                break

            file_group_key, rules = _get_file_group_key(
                additional_rules,
                file_stat.name,
            )
            file_specs = files_matching.setdefault(
                file_group_key, {
                    'count_all': 0,
                    'size_all': 0,
                    'size_minmax': None,
                    'age_minmax': None,
                    'rules': rules,
                    'file_text': {},
                })

            files_matching['default']['count_all'] += 1
            files_matching['default']['size_all'] += file_stat.size

            if file_group_key != 'default':
                file_specs['count_all'] += 1
                file_specs['size_all'] += file_stat.size

            file_specs['size_minmax'] = _update_minmax(file_stat.size, file_specs['size_minmax'])

            age = reftime - file_stat.time
            file_specs['age_minmax'] = _update_minmax(age, file_specs['age_minmax'])

            # Used for long ouput information
            state, text = _check_individual_files(file_specs['rules'], file_stat.size, age)
            if not skip_ok_files or state:
                file_specs['file_text'][file_stat.name] = text
            break

    long_output = []
    include_patterns = [i for i, _e in group_patterns]
    exclude_patterns = [e for _i, e in group_patterns if e != '']

    long_output.append("Include patterns: %s" % ", ".join(include_patterns))

    if exclude_patterns:
        long_output.append("Exclude patterns: %s" % ", ".join(exclude_patterns))

    # Start Checking
    if files_stat_failed:
        yield 1, "Files with unknown stat: %s" % ", ".join(files_stat_failed)

    default_files = files_matching.pop('default')
    check_definition = _define_fileinfo_group_check(default_files, date_inclusion)
    yield from _fileinfo_check_function(
        check_definition,
        default_files['rules'],
        outof_range_txt,
    )

    yield from _fileinfo_check_conjunctions(check_definition, params)

    for expression, file_specs in files_matching.items():
        yield 0, 'Files matching %s' % expression
        check_definition = _define_fileinfo_group_check(file_specs, date_inclusion)
        yield from _fileinfo_check_function(
            check_definition,
            file_specs['rules'],
            outof_range_txt,
            return_metrics=False,
        )
        long_output.append('Files matching %s:' % expression)
        long_output.extend(
            ['[%s] %s' % file_text for file_text in sorted(file_specs['file_text'].items())])

    if not default_files['file_text']:
        yield 0, '\n' + '\n'.join(long_output)
        return

    long_output.append('(Remaining) files in file group:')
    long_output.extend(
        ['[%s] %s' % file_text for file_text in sorted(default_files['file_text'].items())])

    yield 0, '\n' + '\n'.join(long_output)


def _fileinfo_check_function(check_definition, params, outof_range_txt, return_metrics=True):
    infotexts = []
    states = []
    allperfdata = []
    for title, key, value, verbfunc in check_definition:
        if value in [None, ""]:
            continue
        levels = params.get("max" + key, (None, None)) + params.get("min" + key, (None, None))
        state, infotext, perfdata = check_levels(value,
                                                 key if return_metrics else None,
                                                 levels,
                                                 human_readable_func=verbfunc,
                                                 infoname=title)
        states.append(state)
        infotexts.append(infotext)

        # because strings go into infos but not into perfdata
        if isinstance(value, int):
            allperfdata.append(perfdata)
        else:
            allperfdata.append([])

    if outof_range_txt:
        infotexts = [outof_range_txt] + infotexts
        states = [0] * (len(states) + 1)
        allperfdata = [[]] + allperfdata

    for res in zip(states, infotexts, allperfdata):
        yield res


def inventory_fileinfo(parsed):
    return inventory_fileinfo_common(parsed, "single")


def _fileinfo_check_conjunctions(check_definition, params):
    conjunctions = params.get("conjunctions", [])
    for conjunction_state, levels in conjunctions:
        levels = dict(levels)
        match_texts = []
        matches = 0
        for title, key, value, readable_f in check_definition:
            level = levels.get(key)
            if level is not None and value >= level:
                match_texts.append("%s at %s" % (title.lower(), readable_f(level)))
                matches += 1

            level_lower = levels.get("%s_lower" % key)
            if level_lower is not None and value < level_lower:
                match_texts.append("%s below %s" % (title.lower(), readable_f(level_lower)))
                matches += 1

        if matches == len(levels):
            yield conjunction_state, "Conjunction: %s" % " AND ".join(match_texts)


check_info["fileinfo"] = {
    "parse_function": parse_fileinfo,
    "check_function": check_fileinfo,
    "inventory_function": inventory_fileinfo,
    "service_description": "File %s",
    "has_perfdata": True,
    "group": "fileinfo",
}


def inventory_fileinfo_groups(parsed):
    return inventory_fileinfo_common(parsed, "group")


check_info['fileinfo.groups'] = {
    "check_function": check_fileinfo_groups,
    "inventory_function": inventory_fileinfo_groups,
    "service_description": "File group %s",
    "has_perfdata": True,
    "group": "fileinfo-groups",
}
