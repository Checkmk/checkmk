#!/usr/bin/python
# -*- encoding: utf-8; py-indent-offset: 4 -*-
# +------------------------------------------------------------------+
# |             ____ _               _        __  __ _  __           |
# |            / ___| |__   ___  ___| | __   |  \/  | |/ /           |
# |           | |   | '_ \ / _ \/ __| |/ /   | |\/| | ' /            |
# |           | |___| | | |  __/ (__|   <    | |  | | . \            |
# |            \____|_| |_|\___|\___|_|\_\___|_|  |_|_|\_\           |
# |                                                                  |
# | Copyright Mathias Kettner 2014             mk@mathias-kettner.de |
# +------------------------------------------------------------------+
#
# This file is part of Check_MK.
# The official homepage is at http://mathias-kettner.de/check_mk.
#
# check_mk is free software;  you can redistribute it and/or modify it
# under the  terms of the  GNU General Public License  as published by
# the Free Software Foundation in version 2.  check_mk is  distributed
# in the hope that it will be useful, but WITHOUT ANY WARRANTY;  with-
# out even the implied warranty of  MERCHANTABILITY  or  FITNESS FOR A
# PARTICULAR PURPOSE. See the  GNU General Public License for more de-
# tails. You should have  received  a copy of the  GNU  General Public
# License along with GNU Make; see the file  COPYING.  If  not,  write
# to the Free Software Foundation, Inc., 51 Franklin St,  Fifth Floor,
# Boston, MA 02110-1301 USA.

import errno
import os
import collections
import io

import pathlib2

import cmk.utils.debug
import cmk.utils.paths

# Configuration variables in main.mk needed during the actual check
logwatch_dir = cmk.utils.paths.var_dir + '/logwatch'
logwatch_rules = []
logwatch_max_filesize = 500000  # do not save more than 500k of message (configurable)
logwatch_service_output = "default"
logwatch_groups = []

# Deprecated option since 1.6. cmk.base creates a config warning when finding rules
# for this ruleset. Can be dropped with 1.7.
logwatch_patterns = {}

# Variables embedded in precompiled checks
check_config_variables += ["logwatch_dir", "logwatch_max_filesize", "logwatch_service_output"]

logwatch_spool_dir = cmk.utils.paths.var_dir + '/logwatch_spool'

#   .--General-------------------------------------------------------------.
#   |                   ____                           _                   |
#   |                  / ___| ___ _ __   ___ _ __ __ _| |                  |
#   |                 | |  _ / _ \ '_ \ / _ \ '__/ _` | |                  |
#   |                 | |_| |  __/ | | |  __/ | | (_| | |                  |
#   |                  \____|\___|_| |_|\___|_|  \__,_|_|                  |
#   |                                                                      |
#   +----------------------------------------------------------------------+
#   |  General functions, for normal and forwarding logwatch check         |
#   '----------------------------------------------------------------------'


def _extract_error_message(node, line):
    '''Check line for error message
    Return (False, None) if no error message is found,
    (True, error_message) otherwise
    '''
    if not line.startswith("CANNOT READ CONFIG FILE: "):
        return False, None
    error_msg = line[25:]
    if node is None:
        return True, "Error in agent configuration: %s" % error_msg
    return True, "Error in agent configuration on node %s: %s" % (node, error_msg)


def _extract_item_attribute(line):
    '''Check line for next item subsection
    Return (None, None) if no item subsection is found,
    (item, attribute) otherwise
    '''
    if line[:3] != "[[[" or line[-3:] != "]]]":
        return None, None
    logfile_name = line[3:-3]
    if logfile_name.endswith(':missing') or logfile_name.endswith(':cannotopen'):
        return logfile_name.rsplit(':', 1)
    return logfile_name, 'ok'


def parse_logwatch(info):

    parsed = {'errors': [], 'logfiles': {}}
    item_data = {}  # should never be used

    for line in info:
        node = line[0]
        line = " ".join(line[1:])

        is_error, error_msg = _extract_error_message(node, line)
        if is_error:
            parsed['errors'].append(error_msg)
            continue

        item, attribute = _extract_item_attribute(line)
        if item is not None:
            if item not in parsed['logfiles']:
                parsed['logfiles'][item] = {
                    node: {
                        'attr': attribute,
                        'lines': []
                    },
                }
            item_data = parsed['logfiles'][item]
            continue

        if node not in item_data:
            item_data[node] = {'attr': attribute, 'lines': []}

        item_data[node]['lines'].append(line)

    return parsed


_CACHE_IS_CACHE_NEW = {}


def _logwatch_cachefile_path(node):
    return "%s/%s" % (cmk.utils.paths.tcp_cache_dir, node)


def _logwatch_is_cache_new(last_run, node):
    if node is None:
        return True

    if node in _CACHE_IS_CACHE_NEW:
        return _CACHE_IS_CACHE_NEW[node]

    path = _logwatch_cachefile_path(node)
    if not os.path.exists(path):
        raise MKGeneralException("cache not found: %s" % path)

    return _CACHE_IS_CACHE_NEW.setdefault(node, os.stat(path).st_mtime > last_run)


def logwatch_ec_forwarding_enabled(params, item):
    if 'restrict_logfiles' not in params:
        return True  # matches all logs on this host
    else:
        # only logs which match the specified patterns
        for pattern in params['restrict_logfiles']:
            if re.match(pattern, item):
                return True

    return False


def _logwatch_discoverable_items(parsed):
    '''only consider files which are 'ok' on at least one node'''
    return sorted(item for item, item_data in parsed['logfiles'].iteritems()
                  if any(node_data['attr'] == 'ok' for node_data in item_data.itervalues()))


# Splits the number of existing logfiles into
# forwarded (to ec) and not forwarded. Returns a
# pair of forwarded and not forwarded logs.
def _logwatch_select_forwarded(discoverable_items):
    forward_settings = host_extra_conf(host_name(), get_checkgroup_parameters('logwatch_ec', []))
    # Is forwarding enabled in general?
    if not (forward_settings and forward_settings[0]):
        return [], discoverable_items

    forwarded_logs, not_forwarded_logs = [], []
    for item in discoverable_items:
        if logwatch_ec_forwarding_enabled(forward_settings[0], item):
            forwarded_logs.append(item)
        else:
            not_forwarded_logs.append(item)

    return forwarded_logs, not_forwarded_logs


# New rule-stule logwatch_rules in WATO friendly consistent rule notation:
#
# logwatch_rules = [
#   ( [ PATTERNS ], ALL_HOSTS, [ "Application", "System" ] ),
# ]
# All [ PATTERNS ] of matching rules will be concatenated in order of
# appearance.
#
# PATTERN is a list like:
# [ ( 'O',      ".*ssh.*" ),          # Make informational (OK) messages from these
#   ( (10, 20), "login"   ),          # Warning at 10 messages, Critical at 20
#   ( 'C',      "bad"     ),          # Always critical
#   ( 'W',      "not entirely bad" ), # Always warning
# ]
#


# Extracts patterns that are relevant for the current host and item.
# Constructs simple list of pairs: [ ('W', 'crash.exe'), ('C', 'sshd.*test') ]
def logwatch_precompile(hostname, item, _unused):
    # Initialize the patterns list with the logwatch_rules
    params = {"reclassify_patterns": []}

    # This is the new (-> WATO controlled) variable
    rules = service_extra_conf(hostname, item, logwatch_rules)

    for rule in rules:
        if isinstance(rule, dict):
            params["reclassify_patterns"].extend(rule["reclassify_patterns"])
            if "reclassify_states" in rule:
                params["reclassify_states"] = rule["reclassify_states"]
        else:
            params["reclassify_patterns"].extend(rule)

    return params


def logwatch_reclassify(counts, patterns, text, old_level):
    new_level = old_level

    # Reclassify state to another state
    change_state_paramkey = ("%s_to" % old_level).lower()
    if change_state_paramkey in patterns.get("reclassify_states", {}) and\
        patterns["reclassify_states"][change_state_paramkey] != old_level:
        new_level = patterns["reclassify_states"][change_state_paramkey]

    # Reclassify state if a given regex pattern matches
    # A match overrules the previous state->state reclassification
    for level, pattern, _ in patterns.get("reclassify_patterns", []):
        reg = regex(pattern, re.UNICODE)
        if reg.search(text):
            # If the level is not fixed like 'C' or 'W' but a pair like (10, 20),
            # then we count how many times this pattern has already matched and
            # assign the levels according to the number of matches of this pattern.
            if isinstance(level, tuple):
                warn, crit = level
                newcount = counts.setdefault(id(pattern), 0) + 1
                counts[id(pattern)] = newcount
                if newcount >= crit:
                    return 'C'
                elif newcount >= warn:
                    return 'W'
                return 'I'
            return level

    return new_level


def inventory_logwatch_single(parsed):
    discoverable_items = _logwatch_discoverable_items(parsed)
    _forwarded_logs, not_forwarded_logs = _logwatch_select_forwarded(discoverable_items)
    inventory_groups = host_extra_conf(host_name(), logwatch_groups)

    for logfile in not_forwarded_logs:
        if not any(
                logwatch_groups_of_logfile(group_patterns, logfile)
                for group_patterns in inventory_groups):
            yield logfile, None  # file was in no group


def inventory_logwatch_groups(parsed):
    discoverable_items = _logwatch_discoverable_items(parsed)
    _forwarded_logs, not_forwarded_logs = _logwatch_select_forwarded(discoverable_items)
    inventory_groups = host_extra_conf(host_name(), logwatch_groups)
    inventory = {}

    for logfile in not_forwarded_logs:
        for group_patterns in inventory_groups:
            newly_found = logwatch_groups_of_logfile(group_patterns, logfile)
            for group_name, pattern_set in newly_found.iteritems():
                inventory.setdefault(group_name, set()).update(pattern_set)

    for group_name, patterns in inventory.iteritems():
        # Convert pattern containers to lists (sets are not possible in autochecks)
        yield group_name, {"group_patterns": sorted(patterns)}


#.
#   .--Logwatch------------------------------------------------------------.
#   |              _                           _       _                   |
#   |             | | ___   __ ___      ____ _| |_ ___| |__                |
#   |             | |/ _ \ / _` \ \ /\ / / _` | __/ __| '_ \               |
#   |             | | (_) | (_| |\ V  V / (_| | || (__| | | |              |
#   |             |_|\___/ \__, | \_/\_/ \__,_|\__\___|_| |_|              |
#   |                      |___/                                           |
#   +----------------------------------------------------------------------+
#   |  Normal logwatch check                                               |
#   '----------------------------------------------------------------------'


# In case of a precompiled check, params contains the precompiled
# logwatch patterns for the logfile we deal with. If using check_mk
# without precompiled checks, the params must be None an will be
# ignored.
def check_logwatch(item, params, parsed):
    for error_msg in parsed['errors']:
        yield 3, error_msg

    now = time.time()
    last_run = get_item_state("logwatch_last_run_%s" % item, 0)
    set_item_state("logwatch_last_run_%s" % item, now)

    item_data = parsed['logfiles'].get(item, {})

    loglines = []
    for node, node_data in item_data.iteritems():
        if _logwatch_is_cache_new(last_run, node):
            loglines.extend(node_data['lines'])

    found = item in _logwatch_discoverable_items(parsed)
    yield check_logwatch_generic(item, params, loglines, found)


check_info['logwatch'] = {
    'parse_function': parse_logwatch,
    'inventory_function': inventory_logwatch_single,
    'check_function': check_logwatch,
    'service_description': "Log %s",
    'node_info': True,
    'group': 'logwatch',
}

precompile_params['logwatch'] = logwatch_precompile

#.
#   .--logwatch.groups-----------------------------------------------------.
#   |              _                                                       |
#   |             | |_      ____ _ _ __ ___  _   _ _ __  ___               |
#   |             | \ \ /\ / / _` | '__/ _ \| | | | '_ \/ __|              |
#   |             | |\ V  V / (_| | | | (_) | |_| | |_) \__ \              |
#   |             |_| \_/\_(_)__, |_|  \___/ \__,_| .__/|___/              |
#   |                        |___/                |_|                      |
#   '----------------------------------------------------------------------'


def logwatch_group_precompile(hostname, item, params):
    patterns = []
    for line in host_extra_conf(hostname, logwatch_groups):
        for group_name_pattern, pattern in line:
            if group_name_pattern == item:
                patterns.append(pattern)

    if params is None:
        precomped = {}
    else:
        precomped = params.copy()
    precomped["pre_comp_group_patterns"] = patterns
    precomped.update(logwatch_precompile(hostname, item, None))
    return precomped


def _logwatch_instantiate_matched(match, group_name, inclusion):
    num_perc_s = group_name.count("%s")
    matches = [g or "" for g in match.groups()]

    if len(matches) < num_perc_s:
        raise MKGeneralException(
            "Invalid entry in inventory_logwatch_groups: "
            "group name '%s' contains %d times '%%s', but regular expression "
            "'%s' contains only %d subexpression(s)." % \
            (group_name, num_perc_s, inclusion, len(matches)))

    if not matches:
        return group_name, inclusion

    for num, group in enumerate(matches):
        inclusion = instantiate_regex_pattern_once(inclusion, group)
        group_name = group_name.replace("%%%d" % (num + 1), group)
    return group_name % tuple(matches[:num_perc_s]), inclusion


def logwatch_groups_of_logfile(group_patterns, filename):
    found_these_groups = {}
    for group_name, (inclusion, exclusion) in group_patterns:
        inclusion_is_regex = inclusion.startswith("~")
        exclusion_is_regex = exclusion.startswith("~")

        if inclusion_is_regex:
            reg = regex(inclusion[1:])
            incl_match = reg.match(filename)
            if incl_match:
                group_name, inclusion = _logwatch_instantiate_matched(incl_match, group_name,
                                                                      inclusion)
        else:
            incl_match = fnmatch.fnmatch(filename, inclusion)

        if exclusion_is_regex:
            reg = regex(exclusion[1:])
            excl_match = reg.match(filename)
        else:
            excl_match = fnmatch.fnmatch(filename, exclusion)

        if incl_match and not excl_match:
            found_these_groups.setdefault(group_name, set())
            found_these_groups[group_name].add((inclusion, exclusion))

    return found_these_groups


def _logwatch_match_group_patterns(logfile_name, inclusion, exclusion):

    inclusion_is_regex = inclusion.startswith("~")
    if inclusion_is_regex:
        incl_match = regex(inclusion[1:]).match(logfile_name)
    else:
        incl_match = fnmatch.fnmatch(logfile_name, inclusion)
    if not incl_match:
        return False

    exclusion_is_regex = exclusion.startswith("~")
    if exclusion_is_regex:
        excl_match = regex(exclusion[1:]).match(logfile_name)
    else:
        excl_match = fnmatch.fnmatch(logfile_name, exclusion)
    return not excl_match


def check_logwatch_groups(item, params, parsed):
    for error_msg in parsed['errors']:
        yield 3, error_msg

    group_patterns = set(params.get("pre_comp_group_patterns", []))
    for entry in params.get('group_patterns', []):
        group_patterns.add(entry)

    loglines = []
    for logfile_name, item_data in parsed['logfiles'].iteritems():
        for inclusion, exclusion in group_patterns:
            if _logwatch_match_group_patterns(logfile_name, inclusion, exclusion):
                # node info ignored (only used in regular logwatch check)
                for node_data in item_data.itervalues():
                    loglines.extend(node_data['lines'])
                break

    yield check_logwatch_generic(item, params, loglines, True)


check_info['logwatch.groups'] = {
    'check_function': check_logwatch_groups,
    'inventory_function': inventory_logwatch_groups,
    'service_description': "Log %s",
    'node_info': True,
    'group': 'logwatch',
    'includes': ['eval_regex.include']
}

precompile_params['logwatch.groups'] = logwatch_group_precompile

#.


# truncate a file near the specified offset while keeping lines intact
def truncate_by_line(file_path, offset):
    with file_path.open('r+') as handle:
        handle.seek(offset)
        handle.readline()  # ensures we don't cut inside a line
        handle.truncate()


def logwatch_username():
    import getpass
    return getpass.getuser()


def _makedirs_exist_ok(directory):
    try:
        os.makedirs(directory)
    except OSError as exc:
        if exc.errno != errno.EEXIST:
            raise


class LogwatchBlock(object):

    CHAR_TO_STATE = {"O": 0, "W": 1, "u": 1, "C": 2}
    STATE_TO_STR = {0: "OK", 1: "WARN"}

    def __init__(self, header, patterns):
        self._timestamp = header.strip("<>").rsplit(None, 1)[0]
        self.worst = -1
        self.lines = []
        self.last_worst_line = ''
        self.counts = {}  # counting matches of a certain pattern
        self.states_counter = collections.Counter()  # counting lines with a certain state
        self._patterns = patterns or {}

    def finalize(self):
        state_str = LogwatchBlock.STATE_TO_STR.get(self.worst, "CRIT")
        header = u"<<<%s %s>>>\n" % (self._timestamp, state_str)
        return [header] + self.lines

    def add_line(self, line, skip_reclassification):

        try:
            level, text = line.split(None, 1)
        except ValueError:
            level, text = line.strip(), ""

        if not skip_reclassification:
            level = logwatch_reclassify(self.counts, self._patterns, text, level)

        state = LogwatchBlock.CHAR_TO_STATE.get(level, -1)
        self.worst = max(state, self.worst)

        # Save the last worst line of this block
        if max(state, 0) == self.worst:
            self.last_worst_line = text

        # Count the number of lines by state
        if level != '.':
            self.states_counter[level] += 1

        if not skip_reclassification and level != "I":
            self.lines.append(u"%s %s\n" % (level, text))


class LogwatchBlockCollector(object):
    def __init__(self):
        self.worst = 0
        self.last_worst_line = ""
        self._output_lines = []
        self._states_counter = collections.Counter()

    @property
    def size(self):
        return sum(len(line.encode('utf-8')) for line in self._output_lines)

    def __call__(self, block):
        if not block or block.worst <= -1:
            return

        self._states_counter += block.states_counter

        self._output_lines.extend(block.finalize())
        if block.worst >= self.worst:
            self.worst = block.worst
            self.last_worst_line = block.last_worst_line

    def clear_lines(self):
        self._output_lines = []

    def get_lines(self):
        return self._output_lines

    def get_count_info(self):
        expanded_levels = {"O": "OK", "W": "WARN", "u": "WARN", "C": "CRIT"}
        count_txt = ("%d %s" % (count, expanded_levels.get(level, "IGN"))
                     for level, count in self._states_counter.iteritems())
        return "%s messages" % ', '.join(count_txt)


def check_logwatch_generic(item, params, loglines, found):
    import hashlib
    logmsg_dir = pathlib2.Path(logwatch_dir, host_name())

    logmsg_dir.mkdir(parents=True, exist_ok=True)

    logmsg_file_path = logmsg_dir / item.replace("/", "\\")

    # Logfile (=item) section not found and no local file found. This usually
    # means, that the corresponding logfile also vanished on the target host.
    if not found and not logmsg_file_path.exists():
        return (3, "log not present anymore")

    # Get the patterns (either compile or reuse the precompiled ones)
    # Check_MK creates an empty string if the precompile function has
    # not been executed yet. The precompile function creates an empty
    # list when no ruless/patterns are defined. In case of the logwatch.groups
    # checks, params are a tuple with the normal logwatch parameters on the first
    # and the grouping patterns on the second position
    patterns = logwatch_precompile(host_name(), item, None) if params in ('', None) else params

    block_collector = LogwatchBlockCollector()
    current_block = None

    logmsg_file_exists = logmsg_file_path.exists()
    mode = 'r+' if logmsg_file_exists else 'w'
    try:
        logmsg_file_handle = logmsg_file_path.open(mode, encoding='utf-8')
    except IOError as exc:
        raise MKGeneralException("User %s cannot open file for writing: %s" %
                                 (logwatch_username(), exc))

    pattern_hash = hashlib.sha256(repr(patterns).encode()).hexdigest()
    net_lines = 0
    # parse cached log lines
    if logmsg_file_exists:
        # new format contains hash of patterns on the first line so we only reclassify if they
        # changed
        initline = logmsg_file_handle.readline().rstrip('\n')
        if initline.startswith('[[[') and initline.endswith(']]]'):
            old_pattern_hash = initline[3:-3]
            skip_reclassification = old_pattern_hash == pattern_hash
        else:
            logmsg_file_handle.seek(0)
            skip_reclassification = False

        logfile_size = logmsg_file_path.stat().st_size
        if skip_reclassification and logfile_size > logwatch_max_filesize:
            # early out: without reclassification the file wont shrink and if it is already at
            # the maximum size, all input is dropped anyway
            if logfile_size > logwatch_max_filesize * 2:
                # if the file is far too large, truncate it
                truncate_by_line(logmsg_file_path, logwatch_max_filesize)
            return (2, "unacknowledged messages have exceeded max size, "
                    "new messages are dropped (limit %s)" %
                    get_bytes_human_readable(logwatch_max_filesize))

        for line in logmsg_file_handle:
            line = line.rstrip('\n')
            # Skip empty lines
            if not line:
                continue
            elif line.startswith('<<<') and line.endswith('>>>'):
                # The section is finished here. Add it to the list of reclassified lines if the
                # state of the block is not "I" -> "ignore"
                block_collector(current_block)
                current_block = LogwatchBlock(line, patterns)
            elif current_block is not None:
                current_block.add_line(line, skip_reclassification)
                net_lines += 1

        # The last section is finished here. Add it to the list of reclassified lines if the
        # state of the block is not "I" -> "ignore"
        block_collector(current_block)

        if skip_reclassification:
            output_size = logmsg_file_handle.tell()
            # when skipping reclassification, output lines contains only headers anyway
            block_collector.clear_lines()
        else:
            output_size = block_collector.size
    else:
        output_size = 0
        skip_reclassification = False

    header = time.strftime("<<<%Y-%m-%d %H:%M:%S UNKNOWN>>>\n")
    output_size += len(header)
    header = header.decode('utf-8')

    # process new input lines - but only when there is some room left in the file
    if output_size < logwatch_max_filesize:
        current_block = LogwatchBlock(header, patterns)
        for line in loglines:
            current_block.add_line(line, False)
            net_lines += 1
            output_size += len(line.encode('utf-8'))
            if output_size >= logwatch_max_filesize:
                break
        block_collector(current_block)

    # when reclassifying, rewrite the whole file, outherwise append
    if not skip_reclassification and block_collector.get_lines():
        logmsg_file_handle.seek(0)
        logmsg_file_handle.truncate()
        logmsg_file_handle.write(u"[[[%s]]]\n" % pattern_hash)

    for line in block_collector.get_lines():
        logmsg_file_handle.write(line)
    # correct output size
    logmsg_file_handle.close()
    if net_lines == 0 and logmsg_file_exists:
        logmsg_file_path.unlink()

    # if logfile has reached maximum size, abort with critical state
    if logmsg_file_path.exists() and logmsg_file_path.stat().st_size > logwatch_max_filesize:
        return (2, "unacknowledged messages have exceeded max size, "
                "new messages are lost (limit %s)" %
                get_bytes_human_readable(logwatch_max_filesize))

    #
    # Render output
    #

    if block_collector.worst <= 0:
        return (0, "no error messages")

    info = block_collector.get_count_info()
    if logwatch_service_output == 'default':
        info += ' (Last worst: "%s")' % block_collector.last_worst_line

    return block_collector.worst, info


#.
#   .--Event Console Forwarding--------------------------------------------.
#   |        _                           _       _                         |
#   |       | | ___   __ ___      ____ _| |_ ___| |__    ___  ___          |
#   |       | |/ _ \ / _` \ \ /\ / / _` | __/ __| '_ \  / _ \/ __|         |
#   |       | | (_) | (_| |\ V  V / (_| | || (__| | | ||  __/ (__          |
#   |       |_|\___/ \__, | \_/\_/ \__,_|\__\___|_| |_(_)___|\___|         |
#   |                |___/                                                 |
#   +----------------------------------------------------------------------+
#   | Forwarding logwatch messages to event console                        |
#   '----------------------------------------------------------------------'


# OK      -> priority 5 (notice)
# WARN    -> priority 4 (warning)
# CRIT    -> priority 2 (crit)
# context -> priority 6 (info)
# u = Uknown
def logwatch_to_prio(level):
    if level == 'W':
        return 4
    elif level == 'C':
        return 2
    elif level == 'O':
        return 5
    elif level == '.':
        return 6
    return 4


def _logwatch_inventory_mode_rules():
    forward_settings = host_extra_conf(host_name(), get_checkgroup_parameters('logwatch_ec', []))

    merged_rules = {}
    for rule in forward_settings[-1::-1]:
        if isinstance(rule, dict):
            for key, value in rule.items():
                merged_rules[key] = value
        elif isinstance(rule, str):
            return "no", {}  # Configured "no forwarding"

    mode = "single" if merged_rules.get("separate_checks", False) else "groups"
    return mode, merged_rules


def inventory_logwatch_ec(parsed, use_mode):
    discoverable_items = _logwatch_discoverable_items(parsed)
    forwarded_logs, _not_forwarded_logs = _logwatch_select_forwarded(discoverable_items)
    if not forwarded_logs:
        return

    mode, merged_rules = _logwatch_inventory_mode_rules()
    if mode != use_mode:
        return

    if mode == "groups":
        yield None, {"expected_logfiles": forwarded_logs}
        return

    single_log_params = {}
    for key in ["method", "facility", "monitor_logfilelist", "logwatch_reclassify"]:
        if key in merged_rules:
            single_log_params[key] = merged_rules[key]
    for log in forwarded_logs:
        single_log_params["expected_logfiles"] = [log]
        yield log, single_log_params.copy()


def _logwatch_filter_accumulated_lines(item_data):
    # node info ignored (only used in regular logwatch check)
    for node_data in item_data.itervalues():
        for line in node_data['lines']:
            # skip context lines and ignore lines
            if line[0] not in ('.', 'I'):
                yield line


def check_logwatch_ec(item, params, parsed):
    for error_msg in parsed['errors']:
        yield 3, error_msg

    if item:
        # If this check has an item (logwatch.ec_single), only forward the information from this log
        if item not in parsed['logfiles'] or not logwatch_ec_forwarding_enabled(params, item):
            return
        used_logfiles = [item]
    else:
        # Filter logfiles if some should be excluded
        used_logfiles = [
            name for name in parsed['logfiles'] if logwatch_ec_forwarding_enabled(params, name)
        ]

    # Check if the number of expected files matches the actual one
    if params.get('monitor_logfilelist'):
        if 'expected_logfiles' not in params:
            yield 1, "You enabled monitoring the list of forwarded logfiles. " \
                     "You need to redo service discovery."
        else:
            expected = params['expected_logfiles']
            missing = [f for f in expected if f not in used_logfiles]
            if missing:
                yield 1, "Missing logfiles: %s" % (", ".join(missing))

            exceeding = [f for f in used_logfiles if f not in expected]
            if exceeding:
                yield 1, "Newly appeared logfiles: %s" % (", ".join(exceeding))

    # 3. create syslog message of each line
    # <128> Oct 24 10:44:27 Klappspaten /var/log/syslog: Oct 24 10:44:27 Klappspaten logger: asdasas
    # <facility+priority> timestamp hostname logfile: message
    facility = params.get('facility', 17) << 3  # default to "local1"
    messages = []
    cur_time = int(time.time())

    forwarded_logfiles = set([])

    # Keep track of reclassifed lines
    rclfd_total = 0
    rclfd_to_ignore = 0

    logfile_reclassify_settings = {}
    service_level = get_effective_service_level()

    def add_reclassify_settings(settings):
        if isinstance(settings, dict):
            logfile_reclassify_settings["reclassify_patterns"].extend(
                x for x in settings.get("reclassify_patterns"))
            if "reclassify_states" in settings:
                logfile_reclassify_settings["reclassify_states"] = settings["reclassify_states"]
        else:  # legacy configuration
            logfile_reclassify_settings["reclassify_patterns"].extend(x for x in settings)

    for logfile in used_logfiles:
        lines = _logwatch_filter_accumulated_lines(parsed['logfiles'][logfile])

        logfile_reclassify_settings["reclassify_patterns"] = []
        logfile_reclassify_settings["reclassify_states"] = {}

        # Determine logwatch patterns specifically for this logfile
        if params.get("logwatch_reclassify"):
            logfile_settings = service_extra_conf(host_name(), logfile, logwatch_rules)
            for settings in logfile_settings:
                add_reclassify_settings(settings)

        for line in lines:
            rclfd_level = None
            if logfile_reclassify_settings:
                counts = {}  # unused...
                old_level, _text = line.split(" ", 1)
                level = logwatch_reclassify(counts, logfile_reclassify_settings, line[2:],
                                            old_level)
                if level != old_level:
                    rclfd_total += 1
                    rclfd_level = level
                    if level == "I":  # Ignored lines are not forwarded
                        rclfd_to_ignore += 1
                        continue

            msg = '<%d>' % (facility + logwatch_to_prio(rclfd_level or line[0]),)
            msg += '@%s;%d;; %s %s: %s' % (cur_time, service_level, host_name(), logfile, line[2:])

            messages.append(msg)
            forwarded_logfiles.add(logfile)

    try:
        if forwarded_logfiles:
            logfile_info = " from " + ",".join(list(forwarded_logfiles))
        else:
            logfile_info = ""

        result = logwatch_forward_messages(params.get("method"), item, messages)

        yield 0, "Forwarded %d messages%s" % (result.num_forwarded, logfile_info), \
                            [('messages', result.num_forwarded)]

        exc_txt = " (%s)" % result.exception if result.exception else ""

        if result.num_spooled:
            yield 1, "Spooled %d messages%s" % (result.num_spooled, exc_txt)

        if result.num_dropped:
            yield 2, "Dropped %d messages%s" % (result.num_dropped, exc_txt)

    except Exception as exc:
        if cmk.utils.debug.enabled():
            raise
        yield (2, 'Failed to forward messages (%s). Lost %d messages.' % (exc, len(messages)))

    if rclfd_total:
        yield 0, 'Reclassified %d messages through logwatch patterns (%d to IGNORE)' % \
                             (rclfd_total, rclfd_to_ignore)


class LogwatchFordwardResult(object):
    def __init__(self, num_forwarded=0, num_spooled=0, num_dropped=0, exception=None):
        self.num_forwarded = num_forwarded
        self.num_spooled = num_spooled
        self.num_dropped = num_dropped
        self.exception = exception


# send messages to event console
# a) local in same omd site
# b) local pipe
# c) remote via udp
# d) remote via tcp
def logwatch_forward_messages(method, item, messages):
    if not method:
        method = os.getenv('OMD_ROOT') + "/tmp/run/mkeventd/eventsocket"
    elif method == 'spool:':
        method += os.getenv('OMD_ROOT') + "/var/mkeventd/spool"

    if isinstance(method, tuple):
        return logwatch_forward_tcp(method, messages)

    elif not method.startswith('spool:'):
        return logwatch_forward_pipe(method, messages)

    return logwatch_forward_spool_directory(method, item, messages)


# write into local event pipe
# Important: When the event daemon is stopped, then the pipe
# is *not* existing! This prevents us from hanging in such
# situations. So we must make sure that we do not create a file
# instead of the pipe!
def logwatch_forward_pipe(method, messages):
    if not messages:
        return LogwatchFordwardResult()

    sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    sock.connect(method)
    sock.send(('\n'.join(messages)).encode("utf-8") + '\n')
    sock.close()

    return LogwatchFordwardResult(num_forwarded=len(messages))


# Spool the log messages to given spool directory.
# First write a file which is not read into ec, then
# perform the move to make the file visible for ec
def logwatch_forward_spool_directory(method, item, messages):
    if not messages:
        return LogwatchFordwardResult()

    spool_path = method[6:]
    file_name = '.%s_%s%d' % (host_name(), item and item.replace('/', '\\') + '_' or
                              '', time.time())
    _makedirs_exist_ok(spool_path)

    io.open('%s/%s' % (spool_path, file_name), 'w',
            encoding="utf-8").write(('\n'.join(messages)).encode("utf-8") + '\n')
    os.rename('%s/%s' % (spool_path, file_name), '%s/%s' % (spool_path, file_name[1:]))

    return LogwatchFordwardResult(num_forwarded=len(messages))


def logwatch_forward_tcp(method, new_messages):
    # Transform old format: (proto, address, port)
    if not isinstance(method[1], dict):
        method = (method[0], {"address": method[1], "port": method[2]})

    result = LogwatchFordwardResult()

    message_chunks = []

    if logwatch_shall_spool_messages(method):
        message_chunks += logwatch_load_spooled_messages(method, result)

    # Add chunk of new messages (when there are new ones)
    if new_messages:
        message_chunks.append((time.time(), 0, new_messages))

    if not message_chunks:
        return result  # Nothing to process

    try:
        logwatch_forward_send_tcp(method, message_chunks, result)
    except Exception as exc:
        result.exception = exc

    if logwatch_shall_spool_messages(method):
        logwatch_spool_messages(message_chunks, result)
    else:
        result.num_dropped = sum([len(c[2]) for c in message_chunks])

    return result


def logwatch_shall_spool_messages(method):
    return isinstance(method, tuple) and method[0] == "tcp" \
            and isinstance(method[1], dict) and "spool" in method[1]


def logwatch_forward_send_tcp(method, message_chunks, result):
    protocol, method_params = method

    if protocol == 'udp':
        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    elif protocol == 'tcp':
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    else:
        raise NotImplementedError()

    sock.connect((method_params["address"], method_params["port"]))

    try:
        for _time_spooled, _num_spooled, message_chunk in message_chunks:
            while message_chunk:
                try:
                    message = message_chunk[0]
                except IndexError:
                    break  # chunk complete

                sock.send(message.encode("utf-8") + "\n")
                message_chunk.pop(0)  # remove sent message
                result.num_forwarded += 1
    except Exception as exc:
        result.exception = exc
    finally:
        sock.close()


# a) Rewrite chunks that have been processed partially
# b) Write files for new chunk
def logwatch_spool_messages(message_chunks, result):
    path = logwatch_spool_path()

    _makedirs_exist_ok(path)

    # Now write updated/new and delete emtpy spool files
    for time_spooled, num_already_spooled, message_chunk in message_chunks:
        spool_file_path = "%s/spool.%0.2f" % (path, time_spooled)

        if not message_chunk:
            # Cleanup empty spool files
            try:
                os.unlink(spool_file_path)
                continue
            except OSError as exc:
                if exc.errno == errno.ENOENT:
                    continue
                else:
                    raise

        try:
            # Partially processed chunks or the new one
            with io.open(spool_file_path, "w", encoding="utf-8") as handle:
                handle.write(repr(message_chunk))

            result.num_spooled += len(message_chunk)
        except Exception:
            if cmk.utils.debug.enabled():
                raise

            if num_already_spooled == 0:
                result.num_dropped += len(message_chunk)


def logwatch_load_spooled_messages(method, result):
    import ast

    spool_params = method[1]["spool"]

    try:
        spool_files = sorted(os.listdir(logwatch_spool_path()))
    except OSError as exc:
        if exc.errno == errno.ENOENT:
            return []
        else:
            raise

    message_chunks = []

    total_size = 0
    for filename in spool_files:
        path = logwatch_spool_path() + "/" + filename

        # Delete unknown files
        if not filename.startswith("spool."):
            os.unlink(path)
            continue

        time_spooled = float(filename[6:])
        file_size = os.stat(path).st_size
        total_size += file_size

        # Delete fully processed files
        if file_size in [0, 2]:
            os.unlink(path)
            continue

        # Delete too old files by age
        if time_spooled < time.time() - spool_params["max_age"]:
            logwatch_spool_drop_messages(path, result)
            continue

    # Delete by size till half of target size has been deleted (oldest spool files first)
    if total_size > spool_params["max_size"]:
        target_size = int(spool_params["max_size"] / 2.0)

        for filename in spool_files:
            path = logwatch_spool_path() + "/" + filename

            total_size -= logwatch_spool_drop_messages(path, result)
            if target_size >= total_size:
                break  # cleaned up enough

    # Now process the remaining files
    for filename in spool_files:
        path = logwatch_spool_path() + "/" + filename
        time_spooled = float(filename[6:])

        try:
            messages = ast.literal_eval(io.open(path, encoding="utf-8").read())
        except IOError as exc:
            if exc.errno == errno.ENOENT:
                continue
            else:
                raise

        message_chunks.append((time_spooled, len(messages), messages))

    return message_chunks


def logwatch_spool_drop_messages(path, result):
    import ast
    messages = ast.literal_eval(io.open(path, encoding="utf-8").read())
    result.num_dropped += len(messages)

    file_size = os.stat(path).st_size
    os.unlink(path)
    return file_size


def logwatch_spool_path():
    return logwatch_spool_dir + "/" + host_name()


check_info['logwatch.ec'] = {
    'inventory_function': lambda parsed: inventory_logwatch_ec(parsed, "groups"),
    'check_function': check_logwatch_ec,
    'service_description': "Log Forwarding",
    'group': 'logwatch_ec',
    'node_info': True,
    'has_perfdata': True,
}

check_info['logwatch.ec_single'] = {
    'inventory_function': lambda parsed: inventory_logwatch_ec(parsed, "single"),
    'check_function': check_logwatch_ec,
    'service_description': "Log %s",
    'node_info': True,
    'has_perfdata': True,
}
