#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (C) 2019 tribe29 GmbH - License: GNU General Public License v2
# This file is part of Checkmk (https://checkmk.com). It is subject to the terms and
# conditions defined in the file COPYING, which is part of this source code package.

GRAB_USER = False

factory_settings["ps_default_levels"] = {
    "levels": (1, 1, 99999, 99999),
}

ps_info = collections.namedtuple(
    "Process_Info", ('user', 'virtual', 'physical', 'cputime', 'process_id', 'pagefile',
                     'usermode_time', 'kernelmode_time', 'handles', 'threads', 'uptime', 'cgroup'))

ps_info.__new__.__defaults__ = (None,) * len(ps_info._fields)


def minn(a, b):
    if a is None:
        return b
    if b is None:
        return a
    return min(a, b)


def maxx(a, b):
    if a is None:
        return b
    if b is None:
        return a
    return max(a, b)


def ps_info_tuple(entry):
    ps_tuple_re = regex(r"^\((.*)\)$")
    matched_ps_info = ps_tuple_re.match(entry)
    if matched_ps_info:
        return ps_info(*matched_ps_info.group(1).split(","))
    return False


def ps_wato_configured_inventory_rules(invrules):
    inventory_specs = []
    for value in host_extra_conf(host_name(), invrules):
        default_params = value.get('default_params', value)
        if "cpu_rescale_max" not in default_params:
            default_params["cpu_rescale_max"] = None

        inventory_specs.append((
            value['descr'],
            value.get('match'),
            value.get('user'),
            value.get('cgroup', (None, False)),
            value.get('label', {}),
            default_params,
        ))

    return inventory_specs


def inventory_ps_common(invrules, parsed):
    inventory_specs = ps_wato_configured_inventory_rules(invrules)

    inventory = []
    inventory_labels = []
    for line in parsed:
        for servicedesc, pattern, userspec, cgroupspec, labels, default_params in inventory_specs:
            # First entry in line is the node name or None for non-clusters
            process_info, command_line = line[1], line[2:]
            if not process_attributes_match(process_info, userspec, cgroupspec):
                continue
            matches = process_matches(command_line, pattern)
            if not matches:
                continue  # skip not matched lines

            # User capturing on rule
            if userspec == GRAB_USER:
                i_userspec = process_info.user
            else:
                i_userspec = userspec

            i_servicedesc = servicedesc.replace("%u", i_userspec or "")

            # Process capture
            match_groups = matches.groups() if hasattr(matches, 'groups') else ()

            i_servicedesc = replace_service_description(i_servicedesc, match_groups, pattern)

            # Problem here: We need to instantiate all subexpressions
            # with their actual values of the found process.
            inv_params = {
                "process": pattern,
                "match_groups": match_groups,
                "user": i_userspec,
                "cgroup": cgroupspec,
            }

            # default_params is either a clean dict with optional
            # parameters to set as default or - from version 1.2.4 - the
            # dict from the rule itself. In the later case we need to remove
            # the keys that do not specify default parameters
            for key, value in default_params.items():
                if key not in ("descr", "match", "user", "perfdata"):
                    inv_params[key] = value

            inv = (i_servicedesc, inv_params)

            if inv not in inventory:
                inventory.append(inv)
                inventory_labels.append(HostLabels(*[HostLabel(k, v) for k, v in labels.items()]))

    return inventory + inventory_labels


def replace_service_description(service_description, match_groups, pattern):

    # New in 1.2.2b4: All %1, %2, etc. to be replaced with first, second, ...
    # group. This allows a reordering of the matched groups
    # replace all %1:
    description_template, count = re.subn(r'%(\d+)', r'{\1}', service_description)
    # replace plain %s:
    total_replacements_count = count + description_template.count('%s')
    for number in range(count + 1, total_replacements_count + 1):
        description_template = description_template.replace('%s', '{%d}' % number, 1)

    # It is allowed (1.1.4) that the pattern contains more subexpressions
    # then the service description. In that case only the first
    # subexpressions are used as item.
    try:
        # First argument is None, because format is zero indexed
        return description_template.format(None, *(g or "" for g in match_groups))
    except IndexError:
        raise MKGeneralException(
            "Invalid entry in inventory_processes_rules: service description '%s' contains %d "
            "replaceable elements, but regular expression %r contains only %d subexpression(s)." %
            (service_description, total_replacements_count, pattern, len(match_groups)))


def match_attribute(attribute, pattern):
    if not pattern:
        return True

    if pattern.startswith('~'):
        return bool(regex(pattern[1:]).match(attribute))

    return pattern == attribute


def process_attributes_match(process_info, userspec, cgroupspec):

    cgroup_pattern, invert = cgroupspec
    if process_info.cgroup and (match_attribute(process_info.cgroup, cgroup_pattern) is invert):
        return False

    if not match_attribute(process_info.user, userspec):
        return False

    return True


def process_matches(command_line, process_pattern, match_groups=None):

    if not process_pattern:
        # Process name not relevant
        return True

    if process_pattern.startswith("~"):
        # Regex for complete process command line
        reg = regex(process_pattern[1:])  # skip "~"
        m = reg.match(" ".join(command_line))
        if not m:
            return False
        if match_groups:
            # Versions prior to 1.5.0p20 discovered a list, so keep tuple conversion!
            return m.groups() == tuple(match_groups)
        return m

    # Exact match on name of executable
    return command_line[0] == process_pattern


# produce text or html output intended for the long output field of a check
# from details about a process.  the input is expected to be a list (one
# per process) of lists (one per data field) of key-value tuples where the
# value is again a 2-field tuple, first is the value, second is the unit.
# This function is actually fairly generic so it could be used for other
# data structured the same way
def format_process_list(processes, html_output):
    def format_value(value):
        value, unit = value
        if isinstance(value, float):
            return "%.1f%s" % (value, unit)
        return "%s%s" % (value, unit)

    if html_output:
        table_bracket = "<table>%s</table>"
        line_bracket = "<tr>%s</tr>"
        cell_bracket = "<td>%.0s%s</td>"
        cell_seperator = ""

        headers = []
        headers_found = set()

        for process in processes:
            for key, value in process:
                if key not in headers_found:
                    headers.append(key)
                    headers_found.add(key)

        # make sure each process has all fields from the table
        processes_filled = []
        for process in processes:
            dictified = dict(process)
            processes_filled.append([(key, dictified.get(key, "")) for key in headers])
        processes = processes_filled
        header_line = "<tr><th>" + "</th><th>".join(headers) + "</th></tr>"
    else:
        table_bracket = "%s"
        line_bracket = "%s\r\n"
        cell_bracket = "%s %s"
        cell_seperator = ", "
        header_line = ""

    return table_bracket % (header_line + "".join([
        line_bracket %
        cell_seperator.join([cell_bracket % (key, format_value(value))
                             for key, value in process])
        for process in processes
    ]))


# Parse time as output by ps into seconds.
# Example 1: "12:17"
# Example 2: "55:12:17"
# Example 3: "7-12:34:59" (with 7 days)
# Example 4: "7123459" (only seconds, windows)
def parse_ps_time(text):
    if "-" in text:
        tokens = text.split("-")
        days = int(tokens[0] or 0)
        text = tokens[1]
    else:
        days = 0

    day_secs = sum(
        [factor * int(v or 0) for factor, v in zip([1, 60, 3600], reversed(text.split(":")))])

    return 86400 * days + day_secs


# This function is repeated in cmk/gui/plugins/wato/check_parameters/ps.py
# Update that function too until we can import them
def ps_cleanup_params(params):
    # New parameter format: dictionary. Example:
    # {
    #    "user" : "foo",
    #    "process" : "/usr/bin/food",
    #    "warnmin" : 1,
    #    "okmin"   : 1,
    #    "okmax"   : 1,
    #    "warnmax" : 1,
    # }

    # Even newer format:
    # {
    #   "user" : "foo",
    #   "levels" : (1, 1, 99999, 99999)
    # }
    if isinstance(params, (list, tuple)):
        if len(params) == 5:
            procname, warnmin, okmin, okmax, warnmax = params
            user = None
        elif len(params) == 6:
            procname, user, warnmin, okmin, okmax, warnmax = params

        params = {
            "process": procname,
            "levels": (warnmin, okmin, okmax, warnmax),
            "user": user,
        }

    elif any(k in params for k in ['okmin', 'warnmin', 'okmax', 'warnmax']):
        params["levels"] = (
            params.pop("warnmin", 1),
            params.pop("okmin", 1),
            params.pop("okmax", 99999),
            params.pop("warnmax", 99999),
        )

    if "cpu_rescale_max" not in params:
        params["cpu_rescale_max"] = None

    return params


def check_ps_common(item, params, parsed, cpu_cores=1, info_name="Processes", total_ram=None):
    params = ps_cleanup_params(params)

    processes = check_ps_process_capture(parsed, params, cpu_cores)

    yield ps_count_check(processes, params, info_name)

    for memory_state in memory_check(processes, params):
        yield memory_state

    if processes.resident_size and "resident_levels_perc" in params:
        yield memory_perc_check(processes, params, total_ram)

    # CPU
    if processes.count:
        yield cpu_check(processes.percent_cpu, item, params)

    if "single_cpulevels" in params:
        for ps_state in individual_process_check(processes, params):
            yield ps_state

    # only check handle_count if provided by wmic counters
    if processes.handle_count:
        yield handle_count_check(processes, params)

    if processes.min_elapsed is not None:
        yield from uptime_check(processes, params)

    if params.get("process_info", None):
        infotext = "\n" + format_process_list(processes, params["process_info"] == "html")
        yield 0, infotext


def ps_count_check(processes, params, info_name):
    warnmin, okmin, okmax, warnmax = params["levels"]

    state, infotext, perfdata = check_levels(processes.count,
                                             "count", (okmax + 1, warnmax + 1, okmin, warnmin),
                                             human_readable_func=int,
                                             boundaries=(0, None),
                                             infoname=info_name)

    if processes.running_on_nodes:
        infotext += " [running on %s]" % ", ".join(sorted(processes.running_on_nodes))

    return state, infotext, perfdata


def memory_check(processes, params):
    """Check levels for virtual and physical used memory"""
    for size, title, levels, metric in [
        (processes.virtual_size, "virtual", "virtual_levels", "vsz"),
        (processes.resident_size, "physical", "resident_levels", "rss"),
    ]:
        if size == 0:
            continue

        warn_levels, crit_levels = params.get(levels, (None, None))
        status, info_text, perf_data = check_levels(size * 1024,
                                                    None, (warn_levels, crit_levels),
                                                    human_readable_func=get_bytes_human_readable,
                                                    infoname=title)
        yield status, info_text, [(metric, size, warn_levels, crit_levels)]


def memory_perc_check(processes, params, total_ram):
    """Check levels that are in percent of the total RAM of the host"""
    if not total_ram:
        return 3, "percentual RAM levels configured, but total RAM is unknown"

    resident_perc = 100 * float(processes.resident_size * 1024) / total_ram
    return check_levels(resident_perc,
                        None,
                        params["resident_levels_perc"],
                        human_readable_func=get_percent_human_readable,
                        infoname="Percentage of total RAM")


def cpu_check(percent_cpu, item, params):
    """Check levels for cpu utilization from given process"""

    infotext = "CPU"
    warn_cpu, crit_cpu = params.get("cpulevels", (None, None, None))[:2]
    perf_data = [("pcpu", percent_cpu, warn_cpu, crit_cpu)]

    # CPU might come with previous
    if "cpu_average" in params:
        infotext = "CPU: %s" % get_percent_human_readable(percent_cpu)
        now = time.time()
        avg_cpu = get_average("ps.%s.cpu" % item, now, percent_cpu, params["cpu_average"], False)
        infotext += ", %d min average" % params["cpu_average"]
        perf_data.append(("pcpuavg", avg_cpu, warn_cpu, crit_cpu, 0, params["cpu_average"]))
        percent_cpu = avg_cpu  # use this for level comparison

    state, infotext, _ = check_levels(percent_cpu,
                                      None, (warn_cpu, crit_cpu),
                                      human_readable_func=get_percent_human_readable,
                                      infoname=infotext)
    return state, infotext, perf_data


def individual_process_check(processes, params):
    levels = params["single_cpulevels"]
    for p in processes:
        cpu_usage, name, pid = 0.0, None, None

        for the_item, (value, _unit) in p:
            if the_item == "name":
                name = value
            if the_item == "pid":
                pid = value
            elif the_item.startswith("cpu usage"):
                cpu_usage += value

        process_description = name + " with PID %s CPU" % pid if pid else ""
        state, infotext, _ = check_levels(cpu_usage,
                                          None,
                                          levels,
                                          human_readable_func=get_percent_human_readable,
                                          infoname=process_description)
        if state:
            yield state, infotext


def uptime_check(times, params):
    """Check how long the process is running"""
    if times.min_elapsed == times.max_elapsed:
        yield check_levels(
            times.min_elapsed,
            None,
            params.get("max_age", (None, None)) + params.get("min_age", (None, None)),
            human_readable_func=get_age_human_readable,
            infoname="running for",
        )
    else:
        yield check_levels(
            times.min_elapsed,
            None,
            (None, None) + params.get("min_age", (None, None)),
            human_readable_func=get_age_human_readable,
            infoname="youngest running for",
        )
        yield check_levels(
            times.max_elapsed,
            None,
            params.get("max_age", (None, None)),
            human_readable_func=get_age_human_readable,
            infoname="oldest running for",
        )


def handle_count_check(processes, params):
    return check_levels(processes.handle_count,
                        "process_handles",
                        params.get("handle_count", (None, None)),
                        human_readable_func=int,
                        infoname="process handles")


def cpu_rate(counter, now, lifetime):
    try:
        return get_rate(counter, now, lifetime, onwrap=RAISE)
    except MKCounterWrapped:
        return 0


class ProcessAggregator(object):
    """Collects information about all instances of monitored processes"""
    def __init__(self, cpu_cores, params):
        self.cpu_cores = cpu_cores
        self.params = params
        self.virtual_size = 0
        self.resident_size = 0
        self.handle_count = 0
        self.percent_cpu = 0.0
        self.max_elapsed = None
        self.min_elapsed = None
        self.processes = []
        self.running_on_nodes = set()

    def __getitem__(self, item):
        return self.processes[item]

    @property
    def count(self):
        return len(self.processes)

    def append(self, process):
        self.processes.append(process)

    def core_weight(self, is_win):
        cpu_rescale_max = self.params.get('cpu_rescale_max')

        # Rule not set up, only windows scaled
        if cpu_rescale_max is None and not is_win:
            return 1.0

        # Current rule is set. Explicitly ask not to divide
        if cpu_rescale_max is False:
            return 1.0

        # Use default of division
        return 1.0 / self.cpu_cores

    def lifetimes(self, process_info, process):
        # process_info.cputime contains the used CPU time and possibly,
        # separated by /, also the total elapsed time since the birth of the
        # process.
        if '/' in process_info.cputime:
            elapsed_text = process_info.cputime.split('/')[1]
        else:
            # uptime is a windows only value, introduced in Werk 4029. For
            # future consistency should be moved to the cputime entry and
            # separated by a /
            if process_info.uptime:
                elapsed_text = process_info.uptime
            else:
                elapsed_text = None

        if elapsed_text:
            elapsed = parse_ps_time(elapsed_text)
            self.min_elapsed = minn(self.min_elapsed or elapsed, elapsed)
            self.max_elapsed = maxx(self.max_elapsed, elapsed)

            now = time.time()
            creation_time_unix = int(now - elapsed)
            if creation_time_unix != 0:
                process.append((
                    "creation time",
                    (get_timestamp_human_readable(creation_time_unix), ""),
                ))

    def cpu_usage(self, process_info, process):

        now = time.time()

        pcpu_text = process_info.cputime.split('/')[0]

        if ":" in pcpu_text:  # In linux is a time
            total_seconds = parse_ps_time(pcpu_text)
            pid = process_info.process_id
            cputime = cpu_rate("ps_stat.pcpu.%s" % pid, now, total_seconds)

            pcpu = cputime * 100 * self.core_weight(is_win=False)
            process.append(("pid", (pid, "")))

        # windows cpu times
        elif process_info.usermode_time and process_info.kernelmode_time:
            pid = process_info.process_id

            user_per_sec = cpu_rate("ps_wmic.user.%s" % pid, now, int(process_info.usermode_time))
            kernel_per_sec = cpu_rate("ps_wmic.kernel.%s" % pid, now,
                                      int(process_info.kernelmode_time))

            if not all([user_per_sec, kernel_per_sec]):
                user_per_sec = 0
                kernel_per_sec = 0

            core_weight = self.core_weight(is_win=True)
            user_perc = user_per_sec / 100000.0 * core_weight
            kernel_perc = kernel_per_sec / 100000.0 * core_weight
            pcpu = user_perc + kernel_perc
            process.append(("cpu usage (user space)", (user_perc, "%")))
            process.append(("cpu usage (kernel space)", (kernel_perc, "%")))
            process.append(("pid", (pid, "")))

        else:  # Solaris, BSD, aix cpu times
            if pcpu_text == '-':  # Solaris defunct
                pcpu_text = 0.0
            pcpu = float(pcpu_text) * self.core_weight(is_win=False)

        self.percent_cpu += pcpu
        process.append(("cpu usage", (pcpu, "%")))

        if process_info.pagefile:
            process.append(("pagefile usage", (process_info.pagefile, "")))

        if process_info.handles:
            self.handle_count += int(process_info.handles)
            process.append(("handle count", (int(process_info.handles), "")))


def check_ps_process_capture(parsed, params, cpu_cores):

    ps_aggregator = ProcessAggregator(cpu_cores, params)

    userspec = params.get("user")
    cgroupspec = params.get("cgroup", (None, False))

    for line in parsed:
        node_name, process_line = line[0], line[1:]
        process_info, command_line = process_line[0], process_line[1:]

        if not process_attributes_match(process_info, userspec, cgroupspec):
            continue

        if not process_matches(command_line, params.get("process"), params.get('match_groups')):
            continue

        process = []

        if node_name is not None:
            ps_aggregator.running_on_nodes.add(node_name)

        if command_line:
            process.append(("name", (command_line[0], "")))

        # extended performance data: virtualsize, residentsize, %cpu
        if all(process_info[1:4]):
            process.append(("user", (process_info.user, "")))
            process.append(("virtual size", (int(process_info.virtual), "kB")))
            process.append(("resident size", (int(process_info.physical), "kB")))

            ps_aggregator.virtual_size += int(process_info.virtual)  # kB
            ps_aggregator.resident_size += int(process_info.physical)  # kB

            ps_aggregator.lifetimes(process_info, process)
            ps_aggregator.cpu_usage(process_info, process)

        include_args = params.get("process_info_arguments", 0)
        if include_args:
            process.append(("args", (' '.join(command_line[1:])[:include_args], "")))

        ps_aggregator.append(process)

    return ps_aggregator
