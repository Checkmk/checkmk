#!/usr/bin/python
# -*- encoding: utf-8; py-indent-offset: 4 -*-
# +------------------------------------------------------------------+
# |             ____ _               _        __  __ _  __           |
# |            / ___| |__   ___  ___| | __   |  \/  | |/ /           |
# |           | |   | '_ \ / _ \/ __| |/ /   | |\/| | ' /            |
# |           | |___| | | |  __/ (__|   <    | |  | | . \            |
# |            \____|_| |_|\___|\___|_|\_\___|_|  |_|_|\_\           |
# |                                                                  |
# | Copyright Mathias Kettner 2014             mk@mathias-kettner.de |
# +------------------------------------------------------------------+
#
# This file is part of Check_MK.
# The official homepage is at http://mathias-kettner.de/check_mk.
#
# check_mk is free software;  you can redistribute it and/or modify it
# under the  terms of the  GNU General Public License  as published by
# the Free Software Foundation in version 2.  check_mk is  distributed
# in the hope that it will be useful, but WITHOUT ANY WARRANTY;  with-
# out even the implied warranty of  MERCHANTABILITY  or  FITNESS FOR A
# PARTICULAR PURPOSE. See the  GNU General Public License for more de-
# tails. You should have  received  a copy of the  GNU  General Public
# License along with GNU Make; see the file  COPYING.  If  not,  write
# to the Free Software Foundation, Inc., 51 Franklin St,  Fifth Floor,
# Boston, MA 02110-1301 USA.
GRAB_USER = False

factory_settings["ps_default_levels"] = {
    "levels": (1, 1, 99999, 99999),
}

ps_info = collections.namedtuple(
    "Process_Info", ('user', 'virtual', 'physical', 'cputime', 'process_id', 'pagefile',
                     'usermode_time', 'kernelmode_time', 'handles', 'threads', 'uptime'))

ps_info.__new__.__defaults__ = (None,) * len(ps_info._fields)


def ps_info_tuple(entry):
    ps_tuple_re = regex(r"^\((.*)\)$")
    matched_ps_info = ps_tuple_re.match(entry)
    if matched_ps_info:
        return ps_info(*matched_ps_info.group(1).split(","))
    return False


def ps_wato_configured_inventory_rules(invrules):
    inventory_specs = []
    for value in host_extra_conf(host_name(), invrules):
        default_params = value.get('default_params', value)
        if "cpu_rescale_max" not in default_params:
            default_params["cpu_rescale_max"] = None

        inventory_specs.append((value['descr'], value.get('match'), value.get('user'),
                                default_params))

    return inventory_specs


def inventory_ps_common(invrules, parsed):
    inventory_specs = ps_wato_configured_inventory_rules(invrules)

    inventory = []
    for line in parsed:
        for servicedesc, pattern, userspec, default_params in inventory_specs:
            # First entry in line is the node name or None for non-clusters
            process_line = line[1:]
            matches = process_matches(process_line, pattern, userspec)
            if not matches:
                continue  # skip not matched lines

            # User capturing on rule
            if userspec == GRAB_USER:
                i_userspec = process_line[0].user
            else:
                i_userspec = userspec

            i_servicedesc = servicedesc.replace("%u", i_userspec or "")

            # Process capture
            if hasattr(matches, 'groups'):
                match_groups = [g if g else "" for g in matches.groups()]
            else:
                match_groups = []

            i_servicedesc = replace_service_description(i_servicedesc, match_groups, pattern)

            # Problem here: We need to instantiate all subexpressions
            # with their actual values of the found process.
            inv_params = {
                "process": pattern,
                "match_groups": match_groups,
                "user": i_userspec,
            }

            # default_params is either a clean dict with optional
            # parameters to set as default or - from version 1.2.4 - the
            # dict from the rule itself. In the later case we need to remove
            # the keys that do not specify default parameters
            for key, value in default_params.items():
                if key not in ("descr", "match", "user", "perfdata"):
                    inv_params[key] = value

            inv = (i_servicedesc, inv_params)

            if inv not in inventory:
                inventory.append(inv)

    return inventory


def replace_service_description(service_description, match_groups, pattern):

    # New in 1.2.2b4: Alle %1, %2, etc. to be replaced with first, second, ...
    # group. This allows a reordering of the matched groups
    service_description = re.sub(r'%(\d+)', r'{\1}', service_description)

    # First argument is None, because format is zero indexed
    service_description = service_description.format(None, *match_groups)

    num_elements_replace = service_description.count('%s')
    if len(match_groups) < num_elements_replace:
        raise MKGeneralException(
            "Invalid entry in inventory_processes_rules: "
            "service description '%s' contains "
            "%d replaceable elements, but "
            "regular expression '%s' contains only %d subexpression(s)." %
            (service_description, num_elements_replace, pattern, len(match_groups)))

    # It is allowed (1.1.4) that the pattern contains more subexpressions
    # then the service description. In that case only the first
    # subexpressions are used as item.
    service_description = service_description % tuple(match_groups[:num_elements_replace])

    return service_description


def match_user(user, user_pattern):
    if user_pattern:
        if user_pattern.startswith('~'):
            if not regex(user_pattern[1:]).match(user):
                return False

        elif user_pattern != user:
            return False
    return None


def process_matches(process_line, process_pattern, user_pattern, match_groups=None):
    user, command_line = process_line[0].user, process_line[1:]

    if match_user(user, user_pattern) is False:
        return False

    if not process_pattern:
        # Process name not relevant
        return True

    elif process_pattern.startswith("~"):
        # Regex for complete process command line
        reg = regex(process_pattern[1:])  # skip "~"
        m = reg.match(" ".join(command_line))
        if not m:
            return False
        if match_groups:
            return m.groups() == tuple(match_groups)
        return m

    # Exact match on name of executable
    return command_line[0] == process_pattern


# produce text or html output intended for the long output field of a check
# from details about a process.  the input is expected to be a list (one
# per process) of lists (one per data field) of key-value tuples where the
# value is again a 2-field tuple, first is the value, second is the unit.
# This function is actually fairly generic so it could be used for other
# data structured the same way
def format_process_list(processes, html_output):
    def format_value(value):
        value, unit = value
        if isinstance(value, float):
            return "%.1f%s" % (value, unit)
        return "%s%s" % (value, unit)

    if html_output:
        table_bracket = "<table>%s</table>"
        line_bracket = "<tr>%s</tr>"
        cell_bracket = "<td>%.0s%s</td>"
        cell_seperator = ""

        headers = []
        headers_found = set()

        for process in processes:
            for key, value in process:
                if key not in headers_found:
                    headers.append(key)
                    headers_found.add(key)

        # make sure each process has all fields from the table
        processes_filled = []
        for process in processes:
            dictified = dict(process)
            processes_filled.append([(key, dictified.get(key, "")) for key in headers])
        processes = processes_filled
        header_line = "<tr><th>" + "</th><th>".join(headers) + "</th></tr>"
    else:
        table_bracket = "%s"
        line_bracket = "%s\r\n"
        cell_bracket = "%s %s"
        cell_seperator = ", "
        header_line = ""

    return table_bracket % (header_line + "".join([
        line_bracket % cell_seperator.join(
            [cell_bracket % (key, format_value(value))
             for key, value in process])
        for process in processes
    ]))


# Parse time as output by ps into seconds.
# Example 1: "12:17"
# Example 2: "55:12:17"
# Example 3: "7-12:34:59" (with 7 days)
# Example 4: "7123459" (only seconds, windows)
def parse_ps_time(text):
    if "-" in text:
        tokens = text.split("-")
        days = int(tokens[0] or 0)
        text = tokens[1]
    else:
        days = 0

    day_secs = sum(
        [factor * int(v or 0) for factor, v in zip([1, 60, 3600], reversed(text.split(":")))])

    return 86400 * days + day_secs


# This function is repeated in cmk/gui/plugins/wato/check_parameters/ps.py
# Update that function too until we can import them
def ps_cleanup_params(params):
    # New parameter format: dictionary. Example:
    # {
    #    "user" : "foo",
    #    "process" : "/usr/bin/food",
    #    "warnmin" : 1,
    #    "okmin"   : 1,
    #    "okmax"   : 1,
    #    "warnmax" : 1,
    # }

    # Even newer format:
    # {
    #   "user" : "foo",
    #   "levels" : (1, 1, 99999, 99999)
    # }
    if isinstance(params, (list, tuple)):
        if len(params) == 5:
            procname, warnmin, okmin, okmax, warnmax = params
            user = None
        elif len(params) == 6:
            procname, user, warnmin, okmin, okmax, warnmax = params

        params = {
            "process": procname,
            "levels": (warnmin, okmin, okmax, warnmax),
            "user": user,
        }

    elif any(k in params for k in ['okmin', 'warnmin', 'okmax', 'warnmax']):
        params["levels"] = (
            params.pop("warnmin", 1),
            params.pop("okmin", 1),
            params.pop("okmax", 99999),
            params.pop("warnmax", 99999),
        )

    if "cpu_rescale_max" not in params:
        params["cpu_rescale_max"] = None

    return params


def check_ps_common(item, params, parsed, cpu_cores=1, info_name="process", total_ram=None):
    params = ps_cleanup_params(params)

    processes = check_ps_process_capture(parsed, params, cpu_cores)

    yield ps_count_check(processes, params, info_name)

    for memory_state in memory_check(processes, params):
        yield memory_state

    if processes.resident_size and "resident_levels_perc" in params:
        yield memory_perc_check(processes, params, total_ram)

    # CPU
    if processes.count:
        yield cpu_check(processes.percent_cpu, item, params)

    if "single_cpulevels" in params:
        for ps_state in individual_process_check(processes, params):
            yield ps_state

    # only check handle_count if provided by wmic counters
    if processes.handle_count:
        yield handle_count_check(processes, params)

    if processes.min_elapsed is not None:
        yield uptime_check(processes, params)

    if params.get("process_info", None):
        infotext = "\n" + format_process_list(processes, params["process_info"] == "html")
        yield 0, infotext


def upperlevels(value, warn, crit, readable=str):
    infotext = ''
    if value >= crit:
        state = 2
    elif value >= warn:
        state = 1
    else:
        state = 0
    if state:
        infotext = ": (warn/crit at %s/%s)" % tuple(map(readable, (warn, crit)))

    return state, infotext


def boundary_levels(value, warnmin, okmin, okmax, warnmax):
    infotext = ''
    state = 0
    if value > warnmax or value < warnmin:
        state = 2
    elif value > okmax or value < okmin:
        state = 1
    else:
        state = 0
    if state:
        infotext = ": (ok from %d to %d)" % (okmin, okmax)

    return state, infotext


def ps_count_check(processes, params, info_name):
    warnmin, okmin, okmax, warnmax = params["levels"]
    count = processes.count

    perfdata = [("count", count, okmax + 1, warnmax + 1, 0)]
    infotext = "%d %s%s" % (count, info_name, '' if count == 1 else 'es')

    state, warntext = boundary_levels(count, warnmin, okmin, okmax, warnmax)
    infotext += warntext

    if processes.running_on_nodes:
        infotext += " [running on %s]" % ", ".join(sorted(processes.running_on_nodes))

    return state, infotext, perfdata


def memory_check(processes, params):
    """Check levels for virtual and physical used memory"""
    for size, title, levels, metric in [
        (processes.virtual_size, "virtual", "virtual_levels", "vsz"),
        (processes.resident_size, "physical", "resident_levels", "rss"),
    ]:
        if size == 0:
            continue

        infotext = "%s %s" % (get_bytes_human_readable(size * 1024.0), title)
        warn_levels, crit_levels = params.get(levels, (None, None))
        if levels in params:
            state, levelstext = upperlevels(size * 1024, warn_levels, crit_levels,
                                            get_bytes_human_readable)
        else:
            state, levelstext = 0, ""
        yield state, infotext + levelstext, [(metric, size, warn_levels, crit_levels)]


def memory_perc_check(processes, params, total_ram):
    """Check levels that are in percent of the total RAM of the host"""
    warn_perc, crit_perc = params["resident_levels_perc"]
    if not total_ram:
        infotext = "percentual RAM levels configured, but total RAM is unknown"
        state = 3
    else:
        resident_perc = 100 * float(processes.resident_size * 1024) / total_ram
        infotext = "%s of total RAM" % get_percent_human_readable(resident_perc)
        state, levelstext = upperlevels(resident_perc, warn_perc, crit_perc,
                                        get_percent_human_readable)
        infotext += levelstext

    return state, infotext


def cpu_check(percent_cpu, item, params):
    """Check levels for cpu utilization from given process"""

    infotext = "%.1f%% CPU" % percent_cpu
    warn_cpu, crit_cpu = params.get("cpulevels", (None, None, None))[:2]
    perf_data = [("pcpu", percent_cpu, warn_cpu, crit_cpu)]

    # CPU might come with previous
    if "cpu_average" in params:
        now = time.time()
        avg_cpu = get_average("ps.%s.cpu" % item, now, percent_cpu, params["cpu_average"], False)
        infotext += " (%d min average: %.1f%%)" % (params["cpu_average"], avg_cpu)
        perf_data.append(("pcpuavg", avg_cpu, warn_cpu, crit_cpu, 0, params["cpu_average"]))
        percent_cpu = avg_cpu  # use this for level comparison

    state, levelstext = upperlevels(percent_cpu, warn_cpu, crit_cpu,
                                    get_percent_human_readable) if "cpulevels" in params else (0,
                                                                                               "")
    return state, infotext + levelstext, perf_data


def individual_process_check(processes, params):
    warn_cpu_single, crit_cpu_single = params["single_cpulevels"]
    for p in processes:
        cpu_usage, name, pid = 0.0, None, None

        for the_item, (value, _unit) in p:
            if the_item == "name":
                name = value
            if the_item == "pid":
                pid = value
            elif the_item.startswith("cpu usage"):
                cpu_usage += value

        state, levelstext = upperlevels(cpu_usage, warn_cpu_single, crit_cpu_single,
                                        get_percent_human_readable)
        if state:
            process_description = name + " with PID %s" % pid if pid else ""
            infotext = "%.1f%% CPU for %s%s" % (cpu_usage, process_description, levelstext)
            yield state, infotext


def uptime_check(times, params):
    """Check how long the process is running"""
    state = 0
    if times.min_elapsed == times.max_elapsed:
        infotext = "running for %s" % get_age_human_readable(times.min_elapsed)
    else:
        infotext = "youngest running for {}, oldest running for {}".format(
            *map(get_age_human_readable, [times.min_elapsed, times.max_elapsed]))

    if "max_age" in params:
        warn_age, crit_age = params["max_age"]
        state, levelstext = upperlevels(times.max_elapsed, warn_age, crit_age,
                                        get_age_human_readable)
        infotext += levelstext

    return state, infotext


def handle_count_check(processes, params):
    infotext = "%d process handles" % processes.handle_count
    warn_handle, crit_handle = params.get("handle_count", (None, None))
    state, levelstext = upperlevels(processes.handle_count, warn_handle,
                                    crit_handle) if "handle_count" in params else (0, "")
    return state, infotext + levelstext, [("process_handles", processes.handle_count, warn_handle,
                                           crit_handle)]


def cpu_rate(counter, now, lifetime):
    try:
        return get_rate(counter, now, lifetime, onwrap=RAISE)
    except MKCounterWrapped:
        return 0


class ProcessAggregator(object):
    """Collects information about all instances of monitored processes"""

    def __init__(self, cpu_cores, params):
        self.cpu_cores = cpu_cores
        self.params = params
        self.virtual_size = 0
        self.resident_size = 0
        self.handle_count = 0
        self.percent_cpu = 0.0
        self.max_elapsed = None
        self.min_elapsed = None
        self.processes = []
        self.running_on_nodes = set()

    def __getitem__(self, item):
        return self.processes[item]

    @property
    def count(self):
        return len(self.processes)

    def append(self, process):
        self.processes.append(process)

    def core_weight(self, is_win):
        cpu_rescale_max = self.params.get('cpu_rescale_max')

        # Rule not set up, only windows scaled
        if cpu_rescale_max is None and not is_win:
            return 1.0

        # Current rule is set. Explicitly ask not to divide
        if cpu_rescale_max is False:
            return 1.0

        # Use default of division
        return 1.0 / self.cpu_cores

    def lifetimes(self, process_info, process):
        # process_info.cputime contains the used CPU time and possibly,
        # separated by /, also the total elapsed time since the birth of the
        # process.
        if '/' in process_info.cputime:
            elapsed_text = process_info.cputime.split('/')[1]
        else:
            # uptime is a windows only value, introduced in Werk 4029. For
            # future consistency should be moved to the cputime entry and
            # separated by a /
            if process_info.uptime:
                elapsed_text = process_info.uptime
            else:
                elapsed_text = None

        if elapsed_text:
            elapsed = parse_ps_time(elapsed_text)
            self.min_elapsed = min(self.min_elapsed or elapsed, elapsed)
            self.max_elapsed = max(self.max_elapsed, elapsed)

            now = time.time()
            creation_time_unix = int(now - elapsed)
            if creation_time_unix != 0:
                process.append((
                    "creation time",
                    (get_timestamp_human_readable(creation_time_unix), ""),
                ))

    def cpu_usage(self, process_info, process):

        now = time.time()

        pcpu_text = process_info.cputime.split('/')[0]

        if ":" in pcpu_text:  # In linux is a time
            total_seconds = parse_ps_time(pcpu_text)
            pid = process_info.process_id
            cputime = cpu_rate("ps_stat.pcpu.%s" % pid, now, total_seconds)

            pcpu = cputime * 100 * self.core_weight(is_win=False)
            process.append(("pid", (pid, "")))

        # windows cpu times
        elif process_info.usermode_time and process_info.kernelmode_time:
            pid = process_info.process_id

            user_per_sec = cpu_rate("ps_wmic.user.%s" % pid, now, int(process_info.usermode_time))
            kernel_per_sec = cpu_rate("ps_wmic.kernel.%s" % pid, now,
                                      int(process_info.kernelmode_time))

            if not all([user_per_sec, kernel_per_sec]):
                user_per_sec = 0
                kernel_per_sec = 0

            core_weight = self.core_weight(is_win=True)
            user_perc = user_per_sec / 100000.0 * core_weight
            kernel_perc = kernel_per_sec / 100000.0 * core_weight
            pcpu = user_perc + kernel_perc
            process.append(("cpu usage (user space)", (user_perc, "%")))
            process.append(("cpu usage (kernel space)", (kernel_perc, "%")))
            process.append(("pid", (pid, "")))

        else:  # Solaris, BSD, aix cpu times
            if pcpu_text == '-':  # Solaris defunct
                pcpu_text = 0.0
            pcpu = float(pcpu_text) * self.core_weight(is_win=False)

        self.percent_cpu += pcpu
        process.append(("cpu usage", (pcpu, "%")))

        if process_info.pagefile:
            process.append(("pagefile usage", (process_info.pagefile, "")))

        if process_info.handles:
            self.handle_count += int(process_info.handles)
            process.append(("handle count", (int(process_info.handles), "")))


def check_ps_process_capture(parsed, params, cpu_cores):

    ps_aggregator = ProcessAggregator(cpu_cores, params)

    for line in parsed:
        node_name, process_line = line[0], line[1:]
        process_info, command_line = process_line[0], process_line[1:]

        if process_matches(process_line, params.get("process"), params.get("user"),
                           params.get('match_groups')):
            process = []

            if node_name is not None:
                ps_aggregator.running_on_nodes.add(node_name)

            if command_line:
                process.append(("name", (command_line[0], "")))

            # extended performance data: virtualsize, residentsize, %cpu
            if all(process_info[1:4]):
                process.append(("user", (process_info.user, "")))
                process.append(("virtual size", (int(process_info.virtual), "kB")))
                process.append(("resident size", (int(process_info.physical), "kB")))

                ps_aggregator.virtual_size += int(process_info.virtual)  # kB
                ps_aggregator.resident_size += int(process_info.physical)  # kB

                ps_aggregator.lifetimes(process_info, process)
                ps_aggregator.cpu_usage(process_info, process)

            ps_aggregator.append(process)

    return ps_aggregator
